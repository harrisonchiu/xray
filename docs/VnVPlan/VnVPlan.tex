\documentclass[12pt, titlepage]{article}

\usepackage{parskip}
\usepackage{float}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{longtable} 
\hypersetup{
  colorlinks,
  citecolor=blue,
  filecolor=black,
  linkcolor=red,
  urlcolor=blue
}
\usepackage[margin=1in,footskip=0.25in]{geometry}
\usepackage[round]{natbib}

\input{../Common}

\begin{document}

\title{System Verification and Validation Plan for \progname{}}
\author{\authname}
\date{\today}

\maketitle

\pagenumbering{roman}

\section*{Revision History}

\begin{tabularx}{\textwidth}{p{6cm}p{2cm}X}
  \toprule {\bf Date} & {\bf Author} & {\bf Notes}\\
  \midrule
  4 November 2024 & Harrison & Section 4 \\
  4 November 2024 & Hamza & Section 5 \\
  4 November 2024 & Gurnoor & Section 1-3 \\
  4 November 2024 & Jared & Section 3, 5 \\
  4 November 2024 & Ahmad & Section 5 \\
  4 April 2024 & Jared, Harrison & Update VnVPlan to address feedback from Github Issues and TA feedback \\
  \bottomrule
\end{tabularx}

~\\
% \wss{The intention of the VnV plan is to increase confidence in the software.
%   However, this does not mean listing every verification and validation technique
%   that has ever been devised.  The VnV plan should also be a \textbf{feasible}
%   plan. Execution of the plan should be possible with the time and team available.
%   If the full plan cannot be completed during the time available, it can either be
%   modified to ``fake it'', or a better solution is to add a section describing
% what work has been completed and what work is still planned for the future.}

% \wss{The VnV plan is typically started after the requirements stage, but before
%   the design stage.  This means that the sections related to unit testing cannot
%   initially be completed.  The sections will be filled in after the design stage
%   is complete.  the final version of the VnV plan should have all sections filled
% in.}

\newpage

\tableofcontents

\listoftables

\newpage

\section{Symbols, Abbreviations, and Acronyms}

Table 1, includes the definitions and descriptions of all relevant symbols,
abbreviations and acronyms used in this VnV Plan document.

\begin{longtable}[c]{|p{0.3\textwidth}|p{0.7\textwidth}|}
  \hline
  \textbf{Symbol, Abbreviation or Acronym} & \textbf{Definiton or Description} \\ \hline
  \textbf{ML} & Machine Learning: A branch of artificial intelligence that involves the use of algorithms to allow computers to learn from and make predictions based on data. This is a core technology used in the project for analyzing chest X-rays. \\ \hline
  \textbf{DL} & Deep Learning: A subset of machine learning involving neural networks with many layers, used to analyze various types of data, including images. \\ \hline
  \textbf{DICOM} & Digital Imaging and Communications in Medicine: A standard for transmitting, storing, and sharing medical imaging information. It is used to manage medical images in the proposed solution. \\ \hline
  \textbf{CNN} & Convolutional Neural Network: A type of deep learning model specifically designed for processing structured grid data like images, used in the project for chest X-ray analysis. \\ \hline
  \textbf{EHR} & Electronic Health Record: A digital version of a patient's paper chart, used for storing patient information and history that may be integrated with the proposed solution. \\ \hline
  \textbf{API} & Application Programming Interface: A set of rules and protocols for building and interacting with software applications, enabling the integration of the proposed solution with other systems. \\ \hline
  \textbf{MC} & Mandated Constraints: Various constraints placed on the project's proposed solution that must be adhered to throughout the development process. \\ \hline
  \textbf{FR} & Functional Requirement: A requirement that specifies what functionality the project's proposed solution must provide to meet user needs. \\ \hline
  \textbf{NFR} & Nonfunctional Requirement: A requirement that specifies criteria that can be used to judge the operation of a system, rather than specific behaviors (e.g., performance, usability). \\ \hline
  \textbf{BUC} & Business Use Case: A scenario that describes how the proposed solution can be used within a business context to achieve specific goals. \\ \hline
  \textbf{PUC} & Product Use Case: A scenario that details how an individual user will interact with the proposed solution to achieve specific tasks. \\ \hline
  \textbf{MVP} & Minimum Viable Product: A version of the proposed solution that includes only the essential features required to meet the core needs of the users and stakeholders. \\ \hline
  \textbf{MG} & Module Guide \\ \hline
  \textbf{MIS} & Module Interface Specification \\ \hline
  \textbf{PoC} & Proof of Concept \\ \hline
  \textbf{SRS} & Software Requirements Specification \\ \hline
  \textbf{FRTC} & Functional Requirements Test Case \\ \hline
  \textbf{NFRTC} & Nonfunctional Requirements Test Case \\ \hline
  \textbf{VnV} & Verification and Validation \\ \hline
\end{longtable}

\pagebreak
\newpage

\pagenumbering{arabic}

This document outlines the verification and validation (VnV) plan for the project's proposed solution. It begins with a summary of the software's general functions, the objectives of the VnV plan (including in-scope and out-of-scope elements), and references relevant documentation.

It lists the VnV team members and describes the process for verifying the SRS, design, VnV plan, and implementation, including the automated testing tools used. The VnV roadmap starts with verifying the SRS, followed by the design, VnV plan, and implementation. After these steps, the software will undergo validation, with milestones achieved through automated tools and guidance from the project supervisor. It also shows the tests done (system and unit tests) to each major module to verify its functionality and output.

\section{General Information}

% Feedback Addressed:
% - [Content] Focused on multi-class classification model reliability and dataset bias.
% - [Traceability] Made performance measures more explicit and testable.
% - [System/Nonfunctional Test Clarity] Clarified ambiguous terms in validation goals.
% - [HIPAA Implementation] Expanded on compliance procedures, not just references.

\subsection{Summary}
The software being developed is the \textbf{Chest Scan} AI diagnostic tool, designed to assist radiologists by automating the classification of chest X-ray images and generating structured diagnostic reports. The system leverages convolutional neural networks (CNNs) to detect and classify multiple thoracic diseases including pneumonia, cardiomegaly, and effusions within a single scan. This multi-class classification capability enhances diagnostic efficiency and helps radiologists manage large volumes of medical imaging. The tool will include user authentication, heatmap visualization, confidence scoring, and a diagnostic report through a web interface.

\subsection{Objectives}
The V\&V Plan for the Chest Scan system targets three major goals: diagnostic reliability across disease categories, secure handling of sensitive clinical data, and intuitive system usability. Each objective is measurable and traceable to the system's core requirements. They guide testing for both functional correctness and compliance with healthcare regulations.

\begin{enumerate}
  \item \textbf{AI Model Accuracy and Diagnostic Reliability}
    \begin{itemize}
      \item Verify that the CNN model can classify chest X-rays across multiple disease labels (e.g., pneumonia, cardiomegaly, edema) with high precision and recall per class.
      \item Confirm that class-specific false positive and false negative rates remain below thresholds defined in the SRS (e.g., $ <10\%$ FP for cardiomegaly).
      \item Ensure that heatmaps and confidence scores correspond to the regions of the X-ray that support each prediction, enabling radiologist verification.
    \end{itemize}
  \item \textbf{Data Security and Access Control}
    \begin{itemize}
      \item Confirm that all personally identifiable health data is encrypted both at rest and during transmission, using standards such as AES-256 and HTTPS.
      \item Verify that only authorized users (e.g., clinicians, researchers) can access system outputs via token-based authentication.
      \item Validate that the system is aligned with HIPAA and PIPEDA standards by conducting compliance reviews and verifying that audit logs, encryption, and access control procedures are implemented correctly.
    \end{itemize}
  \item \textbf{Usability and System Accessibility}
    \begin{itemize}
      \item Verify that radiologists can intuitively upload images, receive multi-label predictions, view corresponding heatmaps, and download PDF reports.
      \item Confirm that the UI supports readability, keyboard navigation, and mobile responsiveness for accessibility.
      \item Validate seamless integration with imaging systems (e.g., PACS) for DICOM file upload and metadata retrieval.
    \end{itemize}
\end{enumerate}

\subsubsection{Out of Scope for the V\&V Plan}
While essential to future development, the following areas are currently beyond the scope of this V\&V plan:

\begin{itemize}
  \item \textbf{Industry-Standard UI/UX Optimization:} The plan does not include formal usability heuristics testing or A/B interface comparisons.
  \item \textbf{Third-Party Library Validation:} The plan assumes correctness of well-established open-source ML libraries (e.g., PyTorch, TensorFlow).
  \item \textbf{Full Load and Stress Testing:} The MVP is scoped for single-user workflows; multi-user scalability testing will be deferred.
\end{itemize}

\subsection{Challenge Level and Extras}
This project is considered high challenge due to its simultaneous emphasis on medical correctness, model generalizability, regulatory compliance, and usability.

\textbf{AI Complexity:} Building a CNN capable of handling multi-label classification on chest X-rays requires careful dataset curation, bias control, and threshold tuning.

\textbf{Healthcare Compliance:} Ensuring secure data handling practices compliant with HIPAA and PIPEDA adds architectural and legal overhead.

\textbf{Clinical Relevance:} Presenting diagnostic results in a format interpretable by radiologists, including justification visualizations (e.g., heatmaps), imposes a high bar for usability and interpretability.

\subsubsection{Extras}
If project capacity permits, the team will pursue the following enhancements:

\begin{itemize}
  \item \textbf{Usability Survey \& feedback Report:} Designed and conducted a usability survey to gather user feedback on our website, then compiled and analyzed the responses in a structured report for deeper insights.  
  \item \textbf{Research Paper and Documentation:} Full technical documentation and a research summary will be published detailing model design, architecture, training pipeline, and V\&V outcomes.
\end{itemize}

\subsection{Relevant Documentation}
The following deliverables provide traceable, testable definitions for both requirements and architecture:

\begin{itemize}
  \item \textbf{Software Requirements Specification (SRS):} Defines functional (e.g., FR1-FR6) and nonfunctional (e.g., SR1-SR3, AR1-AR3) requirements with fit criteria and rationale.
  \item \textbf{Problem Statement Document:} Articulates the motivation, target stakeholders, and scope of the CNN chest X-ray diagnostic system.
  \item \textbf{Module Guide (MG):} Describes the modular breakdown of the system, including the AI engine, API controller, frontend renderer, and report generator.
  \item \textbf{Module Interface Specification (MIS):} Specifies interfaces, state transitions, and exported functions of each module. Used to define V\&V unit test boundaries.
\end{itemize}


\section{Plan}


In this section, we will lay out the roadmap for ensuring each part of the project meets the intended standards through verification and validation. Responsibilities are assigned to team members for each task within the V\&V process. This plan will address how we will confirm that the project's requirements, design documents, V\&V plan, and final implementation align with the specified criteria. Tools and automated testing solutions that support our verification efforts will also be outlined. Additionally, a detailed process for validating the software's functionality and performance will be included. Our verification tasks will be presented in a structured sequence, moving from requirements analysis to design inspection, implementation testing, and review of the V\&V process itself. Requirements verification will continue throughout development, with design checks occurring as the code is produced. Validation of the implementation will be an ongoing effort, eventually leading to a proof-of-concept phase and final project demonstrations.


\subsection{Verification and Validation Team}

% Feedback Addressed:
% - [Content] Updated roles to reflect new project scope: multi-class CNN performance, dataset bias validation, and model output verification.
% - [Traceability] Clear linkage between team responsibilities and the V&V tasks required for AI model validation.

\begin{table}[h]
  \centering
  \begin{tabular}{|c|p{0.65\textwidth}|}
    \hline
    \textbf{VnV Team Member Name} & \textbf{Summary of Role(s)} \\ \hline

    Amirhossein Sabour & 
    Advisor; oversees V\&V strategy, ensures documentation quality, and provides final validation input. Reviews model performance metrics and compliance with clinical AI validation standards. \\ \hline

    Other Design Teams & 
    Peer reviewers. Raise model validation issues (e.g., poor class-wise recall), documentation inconsistencies, or usability oversights through structured feedback and bug reports. \\ \hline

    Gurnoor Bal & 
    Leads verification of CNN model outputs, including confusion matrix analysis, precision/recall per disease class, and threshold sensitivity testing. Also supports validation of image upload and result display pipelines. \\ \hline

    Jared Paul & 
    Manages dataset quality verification tasks. Reviews for bias, annotation mismatches, and underrepresented disease categories. Conducts fairness and generalization checks. \\ \hline

    Ahmad Hamadi & 
    Responsible for backend logic and API validation. Verifies data flow integrity, image pre-processing, and robustness of prediction-serving endpoints. Also participates in Grad-CAM heatmap correctness validation. \\ \hline

    Hamza Issa & 
    Performs V\&V of the web interface, focusing on frontend validation, usability compliance, and PDF generation. Participates in cross-checking classification results against reports. \\ \hline

    Harrison Chiu & 
    Oversees integration testing and multi-label output validation. Coordinates end-to-end test execution for model prediction, heatmap, report pipeline and leads usability testing sessions with stakeholders. \\ \hline
  \end{tabular}
\end{table}

\pagebreak

\subsection{SRS Verification Plan}

% Feedback Addressed:
% - [Content]: Aligned with new project scope (multi-class CNN); added emphasis on model interpretability, FP/FN detection, and confidence scoring.
% - [NFR]: Expanded security section to refer to specific HIPAA obligations and mitigation strategies.
% - [Spelling/Style]: Avoided use of hard-coded "magic numbers" like \CONFIDENCE_THRESHOLD, using symbolic metrics instead.
% - [Traceability]: Highlighted test-case linkage via traceability matrix construction.

The SRS Verification Plan for the "Chest Scan" diagnostic tool ensures that all specified requirements are complete, accurate, and verifiable, particularly in the context of multi-label disease classification. This section outlines the approach to confirm that the SRS document meets quality standards, providing a solid foundation for subsequent design, implementation, and testing phases.

\subsubsection{Objectives}
The objectives of this SRS Verification Plan are to:
\begin{enumerate}
  \item \textbf{Ensure Requirement Completeness:} Confirm that all essential functionalities, such as multi-class image classification, model confidence estimation, heatmap visualizations, and diagnostic report generation, are fully and clearly specified in the SRS.
  \item \textbf{Validate Requirement Clarity:} Verify that each requirement, including classification performance metrics (e.g., precision, recall, F1-score), is unambiguous and testable.
  \item \textbf{Check Requirement Feasibility:} Assess that all technical features-especially those related to deep learning models-are realistically achievable given dataset availability and system constraints.
  \item \textbf{Verify Traceability:} Ensure that each requirement can be traced through to design modules, test cases, and validation outputs.
\end{enumerate}

\subsubsection{Verification Checklist}
To comprehensively verify the SRS, the following key elements will be checked:
\begin{enumerate}
  \item \textbf{Functional Requirements}
    \begin{itemize}
      \item Disease detection and classification across multiple diagnostic categories (e.g., pneumonia, pleural effusion, cardiomegaly, etc.).
      \item Accuracy thresholds for class-wise detection defined using symbolic constants (e.g., \texttt{MIN\_ACCURACY\_THRESHOLD}).
      \item Confidence score computation and annotation with classification results.
      \item Integration of interpretability tools such as Grad-CAM heatmaps.
      \item Diagnostic report generation with location-based annotations and condition summaries.
    \end{itemize}
  \item \textbf{Nonfunctional Requirements}
    \begin{itemize}
      \item \textbf{Performance:} Symbolic benchmarks for classification latency, throughput, and expected accuracy ranges under load.
      \item \textbf{Security:} HIPAA-compliant handling of sensitive data including encryption-at-rest, secure data transfer, and access control policies.
      \item \textbf{Usability:} UI clarity, response times for clinical review, and interpretability of classification results.
      \item \textbf{Reliability:} Uptime requirements, fallback behaviors in case of model failure or data corruption.
    \end{itemize}
  \item \textbf{Constraints}
    \begin{itemize}
      \item Use of PyTorch and Grad-CAM as mandatory frameworks and tooling.
      \item Compliance with public dataset licenses (e.g., CheXpert, MIMIC-CXR, NIH) and hospital system compatibility (PACS/HIS).
    \end{itemize}
  \item \textbf{Traceability and Identification}
    \begin{itemize}
      \item All requirements assigned unique identifiers (e.g., \texttt{FR1}, \texttt{NFR2}) mapped to modules and future test cases.
      \item Inclusion of backward and forward traceability from requirements to test outputs and system modules.
    \end{itemize}
  \item \textbf{Glossary and Terminology}
    \begin{itemize}
      \item Standardization of clinical terms (e.g., infiltrates, nodules), technical terms (e.g., Grad-CAM, ROC-AUC), and abbreviations.
    \end{itemize}
  \item \textbf{Assumptions and Dependencies}
    \begin{itemize}
      \item Availability of annotated chest X-ray datasets (with multi-class labels) assumed.
      \item Dependence on high-performance hardware (e.g., GPU-enabled environments) for real-time inference.
    \end{itemize}
\end{enumerate}

\subsubsection{Methods}
The following methods will be used to verify the SRS:
\begin{itemize}
  \item \textbf{Requirement Review Meetings:} Cross-functional team discussions with domain experts to confirm all requirements support multi-class clinical use cases.
  \item \textbf{Traceability Matrix Construction:} All SRS items mapped to functional and nonfunctional test cases for validation and unit testing.
  \item \textbf{Cross-Referencing with MG and MIS:} Ensures decomposition of requirements into software modules and well-defined interfaces for each responsibility.
\end{itemize}

\subsubsection{Verification Criteria}
The SRS will be considered verified if:
\begin{itemize}
  \item Requirements meet completeness, consistency, and clarity standards suitable for medical AI tools.
  \item Each requirement is covered in the test case traceability matrix with associated validation criteria (e.g., precision, recall, Grad-CAM heatmap visibility).
  \item All feedback from domain reviewers and stakeholders (e.g., radiologists, AI engineers) is addressed and resolved.
\end{itemize}

By following this updated SRS Verification Plan, the team will ensure that the diagnostic system's architecture supports reliable, explainable, and secure multi-disease classification suitable for healthcare deployment.


\subsection{Design Verification Plan}

% Feedback Addressed:
% - [Content]: Expanded to verify CNN architecture, including layer configs, activation functions, and feature extractors.
% - [Traceability]: Reinforced traceability between modules, interfaces, and requirements.
% - [Spelling/Style]: Preserved detailed structure with no ambiguous phrasing.

This Design Verification Plan focuses on verifying that the system's modular structure and interface specifications, as outlined in the Module Guide (MG) and Module Interface Specification (MIS), align with the requirements in the SRS. Each module's functionality, integration, and adaptability to potential changes will be reviewed to ensure the system is robust, maintainable, and ready for clinical use.

\subsubsection{Objectives}
The objectives of this Design Verification Plan are to:
\begin{enumerate}
  \item \textbf{Verify Modular Consistency with Requirements:} Ensure that each module in the MG fulfills its corresponding SRS requirements, including image processing, multi-class classification, Grad-CAM visualization, and secure access.
  \item \textbf{Validate Information Hiding and Encapsulation:} Confirm that modules encapsulate implementation details (e.g., CNN model parameters, data loaders) to support abstraction and maintainability.
  \item \textbf{Check Interface Compatibility:} Verify that module interfaces in the MIS support accurate data exchange between components like the frontend UI, backend API, and CNN classification engine.
  \item \textbf{Assess Anticipated Changes and Traceability:} Ensure that anticipated design changes (e.g., swapping the model architecture or modifying output schema) are localized and traceable.
\end{enumerate}

\subsubsection{Verification Checklist}
The following checklist covers key design aspects to be verified based on the MG and MIS documents:
\begin{enumerate}
  \item \textbf{Module Decomposition}
    \begin{itemize}
      \item Modules follow the information-hiding principle and align with layered design (e.g., ML layer, data layer, UI layer).
      \item CNN architecture module includes breakdown of input layers, convolutional stacks, activations (e.g., ReLU), pooling layers, and softmax/multi-label classifiers.
      \item Traceability matrix maps SRS classification requirements (e.g., FR1-FR4) to CNN Model and Grad-CAM modules.
    \end{itemize}
  \item \textbf{Module Interfaces (MIS)}
    \begin{itemize}
      \item CNN module input interface accepts preprocessed X-ray tensors with consistent shape (e.g., \texttt{B x 1 x 224 x 224}).
      \item Output interface returns a multi-class probability vector, interpretable labels, and Grad-CAM heatmaps.
      \item Modules export constants like \texttt{CLASS\_LABELS}, \texttt{CONFIDENCE\_THRESHOLD}, ensuring no hard-coded literals.
    \end{itemize}
  \item \textbf{Anticipated and Unlikely Changes}
    \begin{itemize}
      \item Anticipated: Change in CNN architecture (e.g., swapping DenseNet with ResNet) should be confined to the ML module.
      \item Unlikely: Change to data modality (e.g., CT scans instead of X-rays) would affect preprocessing and UI assumptions-documented as unlikely to reduce over-design.
    \end{itemize}
  \item \textbf{Traceability and Use Hierarchy}
    \begin{itemize}
      \item Each design element traced to one or more SRS requirements using the traceability matrix.
      \item UI modules depend on Backend API, which depends on ML model, which in turn depends on DataLoader and Preprocessing modules (clean downward hierarchy).
    \end{itemize}
  \item \textbf{User Interface Design (if applicable)}
    \begin{itemize}
      \item Image upload UI and heatmap visualizations are verified for compatibility with backend output format.
      \item Diagnostic report UI tested for multi-class display layout (i.e., stacked or tabular outputs for multiple disease labels).
    \end{itemize}
\end{enumerate}

\subsubsection{Methods}
The following methods will be used to verify the design:
\begin{itemize}
  \item \textbf{Design Walkthroughs:} Systematic reviews with the team to inspect modular boundaries, data flow, and CNN structure alignment with requirements.
  \item \textbf{Consistency Checks with SRS:} Ensure module responsibilities match SRS requirements, including disease classification, heatmap generation, and frontend reporting.
  \item \textbf{Interface Testing:} Simulate API calls or UI interactions using mock data to verify each module interface behaves as expected.
  \item \textbf{Traceability Matrix Review:} Confirm each FR/NFR is mapped to design modules and test plans for coverage and traceability.
\end{itemize}

\subsubsection{Verification Criteria}
The design is considered verified if:
\begin{itemize}
  \item The CNN model structure, activation pipeline, and outputs align with SRS expectations and clinical interpretability.
  \item Modules demonstrate proper encapsulation of model logic, UI routines, and data transformations.
  \item Interface definitions are consistent, conflict-free, and aligned with MIS documentation.
  \item All traceability links between requirements and modules are complete and validated.
\end{itemize}

This plan ensures the system's design is ready for implementation, supporting flexibility, traceability, and correctness for real-world clinical deployment.

\subsection{Verification and Validation Plan Verification Plan}

% Feedback Addressed:
% - [Traceability]: Emphasized linkages with SRS, MG, and MIS as part of traceability verification.
% - [Content]: No structural gaps identified; content already addresses planning completeness and practicality.
% - [Style]: Style and grammar preserved with clear terminology and structure.

This Verification and Validation (V\&V) Plan Verification Plan outlines the process to ensure that the V\&V document itself is thorough, accurate, and effectively designed to support all project goals. By verifying this V\&V document, we aim to confirm that all sections---objectives, scope, methods, and criteria---are clearly defined and cover the requirements, design, and implementation needs outlined in the SRS, MG, and MIS documents.

\subsubsection{Objectives}
The objectives of this V\&V Plan Verification Plan are to:
\begin{enumerate}
  \item \textbf{Ensure Completeness of Verification and Validation Processes:} Verify that this V\&V Plan includes all necessary sections to thoroughly validate the tool's requirements, design, and implementation.
  \item \textbf{Confirm Clarity and Traceability:} Ensure that each section is clearly written, unambiguous, and easy to follow for all project stakeholders, with traceability to project documents.
  \item \textbf{Validate Feasibility of Methods:} Check that the methods and criteria specified in the V\&V Plan are feasible given project resources and timelines.
  \item \textbf{Identify Gaps and Overlaps:} Ensure that all elements of the system are addressed without unnecessary repetition, minimizing gaps or redundant validations.
\end{enumerate}

\subsubsection{Verification Checklist}
The following checklist identifies key elements in the V\&V Plan to verify for completeness and accuracy:
\begin{enumerate}
  \item \textbf{Objectives and Scope}
    \begin{itemize}
      \item Verify that the V\&V Plan includes well-defined objectives and scope covering all essential elements (SRS, MG, MIS).
      \item Confirm that the plan covers functional and nonfunctional requirements, with a clear boundary on what is out of scope.
    \end{itemize}
  \item \textbf{Verification and Validation Methods}
    \begin{itemize}
      \item Ensure that each section (e.g., SRS Verification Plan, Design Verification Plan) includes clearly defined methods and criteria for verifying requirements, design, and implementation.
      \item Confirm that the V\&V methods align with the constraints and objectives laid out in the project's SRS.
    \end{itemize}
  \item \textbf{Traceability and Consistency}
    \begin{itemize}
      \item Check that the V\&V Plan is traceable to all primary project documents (SRS, MG, MIS) with appropriate references, ensuring alignment and consistency across documents.
      \item Ensure that each V\&V Plan section is traceable back to specific requirements or design aspects from the SRS and MG, ideally via traceability matrices.
    \end{itemize}
  \item \textbf{Feasibility and Practicality of Methods}
    \begin{itemize}
      \item Confirm that each verification and validation method in the V\&V Plan is achievable within the project's resources, timeline, and technical constraints.
      \item Check that tools, data, and stakeholder inputs needed for each V\&V process are specified and accessible.
    \end{itemize}
  \item \textbf{Criteria for Success}
    \begin{itemize}
      \item Ensure that the V\&V Plan defines clear criteria for determining the success of each verification and validation step.
      \item Verify that these criteria are realistic and aligned with project requirements, design goals, and intended system functionality.
    \end{itemize}
\end{enumerate}

\subsubsection{Methods}
To verify the V\&V Plan:
\begin{itemize}
  \item \textbf{Document Review Meetings:} Conduct meetings with project stakeholders and team members to review each section of the V\&V Plan for clarity, feasibility, and traceability.
  \item \textbf{Cross-Referencing with Project Documents:} Check each verification and validation method against the SRS, MG, and MIS to confirm alignment and completeness.
  \item \textbf{Feasibility Assessment:} Evaluate the methods and tools proposed in the V\&V Plan to ensure they are practical and achievable with available resources.
\end{itemize}

\subsubsection{Verification Criteria}
The V\&V Plan will be considered verified if:
\begin{itemize}
  \item All sections are complete, clearly written, and aligned with project documents (SRS, MG, MIS).
  \item The verification and validation methods are feasible and cover all essential elements.
  \item Stakeholders confirm that the V\&V Plan is practical and provides a robust approach for testing and validating the tool.
\end{itemize}

This Verification Plan ensures that the V\&V Plan itself is reliable, thorough, and ready to support a comprehensive validation process for the "Chest Scan" diagnostic tool.


\subsection{Implementation Verification Plan}

% Feedback Addressed:
% - [Content]: Added bias mitigation, multi-class classification checks, and dataset adaptation.
% - [System Test Feedback]: Clarified what is meant by "operates as intended" and "realistic conditions."
% - [Magic Numbers]: All test thresholds will use symbolic constants (e.g., ACC\_THRESHOLD) instead of raw numbers.

This Implementation Verification Plan outlines the steps to confirm that the "Chest Scan" diagnostic tool's final implementation accurately reflects its design, fulfills all specified requirements, and operates as intended in realistic clinical environments. The plan emphasizes evaluating model correctness, user experience, dataset adaptability, and multi-label classification robustness. It also verifies that the system supports interpretability and bias-reduction techniques across various disease classes.

\subsubsection{Objectives}
The objectives of the Implementation Verification Plan are to:
\begin{itemize}
  \item \textbf{Verify Functional Accuracy:} Ensure that each implemented feature---including disease detection, diagnostic report generation, and heatmap generation---performs according to its design and aligns with the SRS functional requirements.
  \item \textbf{Confirm Model Stability and Reliability:} Assess the CNN's stability across training sessions using different dataset splits, ensuring that performance (e.g., accuracy, precision, recall) consistently meets symbolic thresholds (e.g., \texttt{ACC\_THRESHOLD}).
  \item \textbf{Validate Dataset Adaptability and Generalization:} Confirm that the model performs well on real-world datasets (e.g., CheXpert, MIMIC-CXR) beyond its training data, ensuring broad applicability.
  \item \textbf{Ensure Security and Usability Compliance:} Verify encryption protocols, access control logic, and that the user interface is intuitive and accessible for clinical users.
  \item \textbf{Evaluate Bias Mitigation:} Verify that training pipelines implement data balancing, augmentation, or stratification techniques to reduce biases from class imbalance or dataset skew.
\end{itemize}

\subsubsection{Verification Methods}
The following methods will be employed to verify implementation:
\begin{itemize}
  \item \textbf{Unit Testing:} Each backend module, including image preprocessing, classification, report formatting, and UI display components, will be tested using automated unit tests tied to unique SRS identifiers.
  \item \textbf{Integration Testing:} Functional connections between subsystems (e.g., frontend upload $\rightarrow$ backend classifier $\rightarrow$ report UI) will be validated. These tests confirm correct data transfer, error handling, and system communication flows.
  \item \textbf{System Testing:} The entire platform will be tested in a near-production environment. Evaluation will include:
    \begin{itemize}
      \item Accuracy metrics (precision, recall, F1-score) on a holdout dataset.
      \item Classification confusion matrix per disease class.
      \item Model explainability checks using Grad-CAM overlays.
      \item Load testing with multiple uploads to verify concurrency.
      \item HIPAA-aligned encryption and access audits.
    \end{itemize}
\end{itemize}

\subsubsection{Verification Criteria}
The implementation will be considered verified if:
\begin{itemize}
  \item \textbf{Pass Rate:} 100\% of unit and integration tests pass without critical or high-severity errors.
  \item \textbf{Performance Thresholds:} Multi-class CNN meets symbolic metrics such as \texttt{ACC\_THRESHOLD}, \texttt{PREC\_THRESHOLD}, and \texttt{RECALL\_THRESHOLD} for top-priority disease classes.
  \item \textbf{Model Stability:} Results from multiple runs with different seeds and data splits show consistent classification performance within acceptable variance.
  \item \textbf{Bias Detection and Mitigation:} Validation shows no statistically significant performance drop between underrepresented and overrepresented classes (p-value < 0.05).
  \item \textbf{Clinical Usability and Security:} Stakeholders confirm that the UI is intuitive, reports are interpretable, and data access follows security standards (AR2, SR3).
\end{itemize}

By following this Implementation Verification Plan, the project team will ensure that the final deployed system behaves reliably across use cases, meets clinical needs, adapts to real-world data, and remains interpretable and secure throughout operation.

\subsection{Automated Testing and Verification Tools}

% Feedback Addressed:
% - [Content]: Added tools and scripts for classification metrics, bias checks, and robustness testing.
% - [Traceability]: Explained how automated outputs support requirements like SR1 and SR3.
% - [NFR Specificity]: Tools now validate security, fairness, and model reliability with measurable outputs.

Automated testing will ensure consistent verification of the "Chest Scan" diagnostic tool's functionality, classification accuracy, security, and fairness throughout its development. Given the complexity of a multi-class CNN in a clinical setting, automated tools will support reproducibility, precision monitoring, and protection against model drift or biased outputs. Tools will be selected to optimize testing efficiency, track model behavior, and enforce project constraints through a continuous integration pipeline.

\subsubsection{Objectives}
The goals of automated testing are to:
\begin{itemize}
  \item \textbf{Enhance Testing Efficiency:} Use automated testing suites to reduce manual verification time and catch regressions early in the development process.
  \item \textbf{Support Continuous Integration and Deployment (CI/CD):} Automate testing workflows to verify code and model behavior on each update.
  \item \textbf{Track Model Performance and Fairness:} Implement evaluation scripts that monitor classification metrics (e.g., precision, recall), dataset bias, and output disparities across disease labels.
  \item \textbf{Evaluate Adversarial Robustness:} Test model performance against perturbed or low-quality images to ensure reliability under real-world imaging variability.
\end{itemize}

\subsubsection{Tools and Methods}
The following tools and methods will be employed during the project:
\begin{itemize}
  \item \textbf{Unit Testing:}
    \begin{itemize}
      \item \texttt{pytest}, \texttt{unittest}: Validate core functionalities such as preprocessing routines, model inference, and data formatting logic.
      \item Test metrics include symbolic constants like \texttt{ACC\_THRESHOLD} and \texttt{RECALL\_THRESHOLD} defined in alignment with SR1.
    \end{itemize}
  \item \textbf{Model Evaluation Scripts:}
    \begin{itemize}
      \item Custom Python scripts to automatically compute:
        \begin{itemize}
          \item Multi-class confusion matrix and per-class precision/recall/F1-scores.
          \item Grad-CAM heatmap visual inspection and feature localization checks.
          \item Bias detection using demographic parity or subgroup accuracy tests (for future scalability).
        \end{itemize}
      \item Tools like \texttt{scikit-learn}, \texttt{captum}, and \texttt{Fairlearn} will be used for metric calculation and fairness evaluation.
    \end{itemize}
  \item \textbf{UI and Integration Testing:}
    \begin{itemize}
      \item \texttt{Selenium}: Automated interaction testing for the frontend, validating image uploads, report visibility, and user flow.
      \item \texttt{React Testing Library}: Unit tests for frontend components, such as result modals and image previews.
    \end{itemize}
  \item \textbf{Continuous Integration Pipeline:}
    \begin{itemize}
      \item \texttt{GitHub Actions}: Executes linting, test suites, and evaluation scripts on each pull request.
      \item \texttt{Flake8}, \texttt{Black}, \texttt{ESLint}: Enforce Python/JS code style and formatting rules.
      \item Auto-reporting system logs test results and accuracy drops to alert developers of performance degradation.
    \end{itemize}
\end{itemize}

\subsubsection{Success Criteria}
Automated testing will be considered effective if:
\begin{itemize}
  \item The CI pipeline completes without failures across unit, integration, and evaluation stages.
  \item Model performance consistently exceeds defined symbolic thresholds (\texttt{ACC\_THRESHOLD}, \texttt{F1\_THRESHOLD}) during automated testing.
  \item Bias and robustness test scripts identify performance disparities early and allow corrective interventions (e.g., resampling or retraining).
  \item Functional UI tests ensure that no deployment breaks the image upload or report display pipeline.
\end{itemize}

Through this automation framework, the project will maintain high-quality, reproducible testing while improving classification accuracy, system robustness, and fairness in clinical AI diagnostics.


\subsection{Software Validation Plan}

% Feedback Addressed:
% - [Content]: Expanded validation scope to include multi-class CNN metrics (per-class accuracy, confusion matrix).
% - [Traceability]: Linked validation to SR0 (accuracy reporting), SR1 (model tuning), and FR1-FR4 (diagnostic performance).
% - [Usability Testing]: Maintained radiologist feedback loop to validate interface functionality and clinical realism.

This Software Validation Plan outlines the approach to confirm that the "Chest Scan" tool meets all functional and nonfunctional requirements, particularly in its role as a multi-class classification system for chest X-ray diagnostics. The plan incorporates quantitative and qualitative methods to ensure accuracy, fairness, usability, and clinical applicability.

External datasets (e.g., MIMIC-CXR, CheXpert) will be used to validate classification performance across multiple disease categories, while user testing with radiologists will provide real-world feedback on system usability and workflow integration.

\subsubsection{Objectives}
The objectives of the Software Validation Plan are to:
\begin{itemize}
  \item \textbf{Validate Requirement Alignment:} Confirm that all functional requirements (FR1-FR4) and nonfunctional requirements (SR0, SR1, SR3) are fulfilled through structured task-based evaluations and stakeholder reviews.
  \item \textbf{Assess Multi-Class Classification Performance:} Use independent validation sets to assess precision, recall, F1-score, and confusion matrix per disease class, ensuring that model outputs meet diagnostic standards.
  \item \textbf{Gather Stakeholder Feedback:} Leverage Rev 0 and Rev 1 demonstration sessions with supervisors, clinicians, and developers to incorporate iterative feedback and refine the tool.
  \item \textbf{Confirm Clinical Usability:} Validate that the web interface and reporting pipeline align with radiologist workflows, supporting intuitive review of AI-generated reports and Grad-CAM visualizations.
\end{itemize}

\subsubsection{Validation Methods}
The following methods will be employed to confirm software effectiveness:
\begin{itemize}
  \item \textbf{Task-Based Inspections:} Perform test cases simulating real-world scenarios such as uploading chest X-rays, viewing disease predictions, and interpreting Grad-CAM heatmaps. These tasks ensure diagnostic and interpretability goals are met.
  \item \textbf{Model Validation on External Datasets:} Run the trained CNN on held-out CheXpert and MIMIC samples and compute multi-class confusion matrices, per-class F1-scores, and overall accuracy to evaluate alignment with SR0 and SR1 thresholds.
  \item \textbf{Demo and Review Sessions:} Present working implementations in Rev 0 and Rev 1, gathering feedback from supervisors and potentially clinical advisors to assess requirement completeness.
  \item \textbf{User Testing with Radiologists:} Conduct observational usability studies, asking participants to navigate the tool, interpret results, and report on clarity, confidence, and workflow impact.
\end{itemize}

\subsubsection{Validation Criteria}
The software will be considered validated if:
\begin{itemize}
  \item CNN model achieves symbolic accuracy and precision thresholds (e.g., \texttt{ACC\_THRESHOLD} $\geq 0.90$) across disease classes on independent datasets (SR0).
  \item Feedback from stakeholders (Rev 0/1) confirms compliance with both functional and nonfunctional expectations.
  \item Radiologist usability feedback indicates that the interface is interpretable, reliable, and clinically usable in simulated workflows.
\end{itemize}

This validation plan ensures the tool's real-world readiness through comprehensive evaluation of both performance metrics and user experience, aligning with the project's clinical goals and SRS-defined criteria.

% Section 4: System Tests

\section{System Tests}

% Feedback addressed:
% - Expanded test cases to cover multi-class classification accuracy and robustness (System Test for Functional Requirements)
% - Removed outdated diffusion model-related tests (FRTC3, FRTC4) (Project Scope Alignment)
% - Replaced hardcoded '\CONFIDENCE_THRESHOLD' with symbolic constant \ACCURACYTHRESHOLD{} (Content Quality)
% - Kept test case labeling and format identical to original VnV Plan (Formatting Consistency)

\newcommand{\ACCURACYTHRESHOLD}{90\%}  % Symbolic constant replacing magic number

This subsection presents the system test cases categorized by areas of functionality. Each test validates a key feature described in the functional requirements. Tests span input handling, multi-class disease identification, report generation, visualization, user access, data protection, and authorization. References to SRS functional requirements are included in the derivations.

\subsection{Tests for Functional Requirements}

\subsubsection{Handling Input Data}

\textbf{FRTC1}\\
\textbf{Title:} Chest X-ray Image Input Acceptance\\
\textbf{Control:} Manual\\
\textbf{Initial State:} The system is initialized and ready to receive input through the web interface.\\
\textbf{Input:} A valid chest X-ray image in supported formats (DICOM, JPEG, or PNG).\\
\textbf{Output:} The image is successfully accepted, read, and queued for analysis with no errors or crashes.\\
\textbf{Test Case Derivation:} The test confirms the system meets baseline input handling by successfully accepting valid file types. The expected output validates the upload and queuing logic for correctly formatted chest X-ray images without backend or UI errors.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Launch the system and ensure it's ready to receive inputs.
  \item Use the upload interface to provide a valid chest X-ray image.
  \item Confirm the image is accepted into the system for processing.
  \item Monitor for any frontend or backend anomalies during the upload.
\end{enumerate}

\vspace{1em}

\textbf{FRTC2}\\
\textbf{Title:} Invalid Chest X-ray Image Format Rejection\\
\textbf{Control:} Manual\\
\textbf{Initial State:} The system is initialized and awaiting file upload.\\
\textbf{Input:} A file in an unsupported or corrupted format (e.g., .txt, .docx, or corrupted .jpg).\\
\textbf{Output:} The system rejects the file with a descriptive error message and logs the failure.\\
\textbf{Test Case Derivation:} The test ensures the system handles invalid or malformed inputs gracefully by displaying clear user-facing errors and suppressing unintended backend behavior. The expected output confirms validation safeguards are enforced to block non-image files or corrupted uploads.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Attempt to upload invalid image files.
  \item Confirm that the UI blocks the upload and displays an appropriate error message.
  \item Verify error handling is logged in the system backend.
  \item Ensure no processing pipeline is triggered.
\end{enumerate}

\vspace{1em}


\subsubsection{FR2}

\textbf{FRTC3}\\
\textbf{Title:} Image Preprocessing Enforcement for CNN Compatibility\\
\textbf{Control:} Manual\\
\textbf{Initial State:} System is running and ready for image upload.\\
\textbf{Input:} A raw chest X-ray image with incorrect resolution and color format.\\
\textbf{Output:} The system resizes to MODEL\_INPUT\_DIMENSIONS, normalizes pixel values, converts to grayscale if needed, and logs errors for failures.\\
\textbf{Test Case Derivation:} The expected output is justified by the CNN model's strict input requirements-ensuring valid data improves model accuracy and prevents runtime errors.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Upload an unprocessed color X-ray image.
  \item Confirm the system performs resizing, normalization, and grayscale conversion.
  \item Upload a malformed image to verify error logging and halt in processing.
\end{enumerate}

\vspace{1em}

\subsubsection{FR3}



\textbf{FRTC4}\\
\textbf{Title:} Disease Classification with Confidence Scores\\
\textbf{Control:} Automatic (with manual verification)\\
\textbf{Initial State:} CNN model is deployed and active.\\
\textbf{Input:} Chest X-ray images from the validation dataset.\\
\textbf{Output:} For each image, the system returns disease predictions with confidence scores greater than 0 and less than 1; per-class accuracy meets or exceeds 60\%.\\
\textbf{Test Case Derivation:} Predictions must include both labels and confidence scores to meet the fit criterion, while model accuracy above 60\% ensures diagnostic reliability.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Load a labeled validation dataset.
  \item Run the CNN model to obtain disease predictions and confidence scores.
  \item Verify the format and range of confidence scores.
  \item Compare predictions against ground truth labels.
  \item Calculate accuracy and confirm it meets or exceeds 60\%.
\end{enumerate}

\vspace{1em}
\subsubsection{FR4}
\textbf{FRTC5}\\
\textbf{Title:} Ambiguous Image Classification (Multi-Label)\\
\textbf{Control:} Automatic (verified manually)\\
\textbf{Initial State:} Model and supporting UI loaded.\\
\textbf{Input:} Chest X-ray showing signs of multiple conditions (e.g., cardiomegaly + pleural effusion).\\
\textbf{Output:} Model correctly assigns multiple labels with balanced precision and recall across classes.\\
\textbf{Test Case Derivation:} Based on updated SRS scope (multi-label classification support).\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Use an ambiguous image with verified multiple diagnoses.
  \item Run the classifier and extract all predicted labels.
  \item Calculate precision, recall, and F1 for each label.
  \item Ensure metrics meet performance thresholds \( \geq ACCURACY\_THRESHOLD \).
\end{enumerate}
\vspace{1em}

\subsubsection{FR5}
\textbf{FRTC6}\\
\textbf{Title:} Display of Predictions and Heatmaps\\
\textbf{Control:} Manual\\
\textbf{Initial State:} System has completed classification for an uploaded image.\\
\textbf{Input:} A classified chest X-ray image.\\
\textbf{Output:} The interface displays disease labels, confidence scores, and heatmaps clearly within MAX\_RENDER\_TIME.\\
\textbf{Test Case Derivation:} Ensures readability and timeliness of visual outputs, matching fit criterion for interpretability.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Upload an image and wait for classification to complete.
  \item Observe the rendered output on the results screen.
  \item Confirm labels, scores, and heatmaps are visible and rendered within MAX\_RENDER\_TIME.
  \item Manually assess layout and readability.
\end{enumerate}

\vspace{1em}
\subsubsection{FR6}
\textbf{FRTC7}\\
\textbf{Title:} Diagnostic Report Download\\
\textbf{Control:} Manual\\
\textbf{Initial State:} A user is logged in and has a classified image result.\\
\textbf{Input:} User clicks "Download Report".\\
\textbf{Output:} Report file downloads with prediction, heatmap, and findings.\\
\textbf{Test Case Derivation:} Confirms output includes required contents in a retrievable format, supporting traceability.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Log in and upload an image.
  \item Wait for predictions and visuals.
  \item Click "Download Report".
  \item Open file and verify it contains predictions, heatmap, and text summary.
\end{enumerate}

\vspace{1em}
\subsubsection{FR6}
\textbf{FRTC8}\\
\textbf{Title:} Report Storage Security and Retrieval\\
\textbf{Control:} Manual\\
\textbf{Initial State:} A previous diagnostic session has been completed.\\
\textbf{Input:} User accesses dashboard and attempts to retrieve prior report.\\
\textbf{Output:} Report is retrieved from encrypted storage (AES-256) and associated with the session.\\
\textbf{Test Case Derivation:} Validates backend compliance with encryption and user-based indexing for secure access.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Upload an image, generate a report, and log out.
  \item Log back in using the same account/session.
  \item Navigate to dashboard and access stored report.
  \item Verify successful decryption and session-specific access.
\end{enumerate}

\vspace{1em}
\subsubsection{FR7}
\textbf{FRTC9}\\
\textbf{Title:} Authentication with Valid and Invalid Credentials\\
\textbf{Control:} Manual\\
\textbf{Initial State:} Login screen is presented.\\
\textbf{Input:} Correct and incorrect username/password combinations.\\
\textbf{Output:} Access granted for valid credentials; access denied and logged for invalid attempts.\\
\textbf{Test Case Derivation:} Ensures only authorized users gain entry and failed logins are audited.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Enter valid credentials and confirm access.
  \item Log out and enter incorrect credentials.
  \item Confirm access is denied.
  \item Check backend logs for authentication failure entry.
\end{enumerate}

\vspace{1em}
\subsubsection{FR7}
\textbf{FRTC10}\\
\textbf{Title:} Role-Based Access to Restricted Features\\
\textbf{Control:} Manual\\
\textbf{Initial State:} User is logged in.\\
\textbf{Input:} User with limited permissions attempts to access restricted modules.\\
\textbf{Output:} Features are hidden or blocked based on user role.\\
\textbf{Test Case Derivation:} Verifies that sensitive functions are only accessible by users with appropriate permissions.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Log in with a basic user role.
  \item Attempt to access admin-only dashboard or reports.
  \item Confirm UI hides or disables access.
  \item Attempt direct URL access and verify rejection.
\end{enumerate}

\vspace{1em}
\subsubsection{FR8}
\textbf{FRTC11}\\
\textbf{Title:} Secure API Image Classification Request\\
\textbf{Control:} Automatic (via script or client request)\\
\textbf{Initial State:} API endpoint is live and accepting requests.\\
\textbf{Input:} A valid POST request with a DICOM image and valid token.\\
\textbf{Output:} Returns JSON with disease labels and confidence scores.\\
\textbf{Test Case Derivation:} Confirms API behaves as expected for compliant clients, providing structured output.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Send POST request with valid token and image.
  \item Capture response payload.
  \item Confirm JSON structure includes disease names and score values.
  \item Validate response format and token usage.
\end{enumerate}

\vspace{1em}
\subsubsection{FR9}
\textbf{FRTC9}\\
\textbf{Title:} Invalid Upload Error Handling\\
\textbf{Control:} Manual\\
\textbf{Initial State:} Upload form is visible to the user.\\
\textbf{Input:} Unsupported or corrupted image file.\\
\textbf{Output:} System displays clear error message and allows retry.\\
\textbf{Test Case Derivation:} Ensures graceful error handling and user guidance on failure.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Attempt to upload a .txt or corrupted .jpg file.
  \item Observe UI for error message.
  \item Confirm message explains the issue and retry option is presented.
\end{enumerate}

\vspace{1em}
\subsubsection{FR10}
\textbf{FRTC10}\\
\textbf{Title:} Diagnostic Result Timestamp Logging\\
\textbf{Control:} Manual\\
\textbf{Initial State:} User has completed multiple uploads.\\
\textbf{Input:} User opens dashboard to view past results.\\
\textbf{Output:} Each entry shows image, prediction, probability, timestamp, and links.\\
\textbf{Test Case Derivation:} Verifies historical tracking and completeness of displayed diagnostic data.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Upload multiple images and generate predictions.
  \item Open dashboard view.
  \item Confirm each result includes timestamp, score, image, and report/heatmap link.
\end{enumerate}

\subsection{Tests for Nonfunctional Requirements}

% Feedback addressed:
% - Included interpretability, latency, and fairness-related tests as required.
% - Expanded test case depth and included stakeholder evaluation for UI-related checks.
% - Ensured traceability to updated SRS nonfunctional requirements.
% - Maintained format consistent with previous sections.

This subsection includes test cases that evaluate the system's appearance, usability, interpretability, and performance, as described in the SRS nonfunctional requirements. These tests are essential to ensuring the system is accessible, trustworthy, and usable in real-world settings.

\subsubsection{Appearance Requirements}

\textbf{NFRTC1}\\
\textbf{Title:} Verification of Calming Color Scheme\\
\textbf{Control:} Manual\\
\textbf{Initial State:} The web application is deployed and accessible from a browser.\\
\textbf{Input:} A user accesses the live system on a browser under default theme settings.\\
\textbf{Output:} The UI displays a cool and calming color palette, primarily using white and soft blue tones.\\
\textbf{Test Case Derivation:} A calming palette enhances usability by improving visual comfort and reducing fatigue, making it easier to interpret outputs.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Open the application in a modern browser under standard lighting.
  \item Review the color palette used across login, dashboard, and report views.
  \item Record the dominant background and highlight colors.
  \item Ask general users to rate visual comfort and clarity on a 5-point scale.
  \item Pass criteria: At least 80\% of users rate visual experience 4 or higher.
\end{enumerate}

\vspace{1em}

\vspace{1em}
\textbf{NFRTC2}\\
\textbf{Title:} Consistency of Simple and Uncluttered Style\\
\textbf{Control:} Manual\\
\textbf{Initial State:} Web app is operational and fully loaded with all components.\\
\textbf{Input:} Navigation across multiple interface modules (login, dashboard, image viewer, report viewer).\\
\textbf{Output:} UI layout remains consistent and free of clutter; visual hierarchy is preserved.\\
\textbf{Test Case Derivation:} Consistency in layout and spacing ensures users can navigate the app without confusion, improving overall usability.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Access each core module of the UI as a user.
  \item Evaluate font styles, spacing, element alignment, and visual consistency across modules.
  \item Perform a walkthrough with 3 representative users.
  \item Track time taken to locate a core function (e.g., report viewer).
  \item Confirm no modules exhibit layout shifts, overlap, or off-brand UI elements.
  \item Success metric: All users navigate without confusion and UI passes visual QA checklist.
\end{enumerate}

\vspace{1em}
\subsubsection{Usability and Humanity Requirements}

\textbf{NFRTC3}\\
\textbf{Title:} Ease of Performing Core Functionality Without Guidance\\
\textbf{Control:} Manual\\
\textbf{Initial State:} The web application is deployed and accessible via a browser; users are interacting with it for the first time.\\
\textbf{Input:} Untrained users attempt to upload a chest X-ray and request classification using the tool.\\
\textbf{Output:} At least 80\% of users complete the task within MAX\_TASK\_DURATION without requiring assistance.\\
\textbf{Test Case Derivation:} The test confirms that intuitive UI design enables users to perform core functions with minimal learning.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Recruit 5-7 first-time users (not involved in design).
  \item Instruct them to upload a chest X-ray image and trigger the diagnosis feature without guidance or tutorial.
  \item Time the task and record completion success/failure.
  \item Observe confusion points or UI obstacles.
  \item Validate success if at least 80\% complete the task in under MAX\_TASK\_DURATION unaided.
  \item Collect optional feedback via Likert-scale survey (e.g., "The task was easy to complete.").
\end{enumerate}

\vspace{1em}

\subsubsection{Usability and Humanity Requirements}

\textbf{NFRTC4}\\
\textbf{Title:} Intuitiveness and Memorability of the Software\\
\textbf{Control:} Manual\\
\textbf{Initial State:} A cohort of users has completed 1-2 guided sessions with the software.\\
\textbf{Input:} Users attempt to navigate and use the system independently after a one-month break.\\
\textbf{Output:} Users are able to recall navigation and perform core actions (e.g., upload, view report) with no external aid.\\
\textbf{Test Case Derivation:} The output reflects long-term usability, as users who remember workflows are less likely to rely on retraining or support.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Provide 1-2 structured walkthroughs of the app to a test group.
  \item Wait 4 weeks without further exposure.
  \item Ask users to independently upload an image and access the report dashboard.
  \item Observe interactions and task success.
  \item Survey user experience, focusing on recall of navigation flow and interface elements.
  \item Pass criteria: At least 80\% of participants complete all steps correctly with minimal hesitation.
\end{enumerate}

\vspace{1em}

\textbf{NFRTC5}\\
\textbf{Title:} Learning Curve Assessment\\
\textbf{Control:} Manual\\
\textbf{Initial State:} System is deployed with a functional user interface, and no help prompts are enabled.\\
\textbf{Input:} First-time users try to perform a full diagnosis cycle (upload, classify, view result).\\
\textbf{Output:} Users complete the cycle independently within MAX\_TASK\_DURATION, with high confidence.\\
\textbf{Test Case Derivation:} Successful completion without help demonstrates that minimal training is required to use the system effectively.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Introduce new users to the platform without providing any tutorials or hints.
  \item Ask them to diagnose a chest X-ray image.
  \item Record time taken and whether they required external help.
  \item Collect post-task confidence ratings (e.g., "How confident were you in your actions?" on a 5-point scale).
  \item Define success if majority complete within MAX\_TASK\_DURATION and rate confidence $\geq$ 4.
\end{enumerate}

\vspace{1em}

\textbf{NFRTC6}\\
\textbf{Title:} Realism of Generated Chest X-Ray Images\\
\textbf{Control:} Manual (with user evaluation)\\
\textbf{Initial State:} The diffusion model is operational and capable of generating synthetic chest X-ray images.\\
\textbf{Input:} A set of generated chest X-ray images using the trained model.\\
\textbf{Output:} Users are unable to consistently distinguish between real and generated images, indicating high visual realism.\\
\textbf{Test Case Derivation:} Accurate visual mimicry ensures synthetic images serve as valid training or testing inputs, making the system trustworthy for downstream tasks.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Generate a dataset of at least 20 synthetic chest X-ray images.
  \item Mix with 20 real images from public datasets.
  \item Ask 3 or more users to label each image as "Real" or "Generated."
  \item Compute accuracy of identifying source.
  \item Success if correct classification rate $\leq$ 60\%.
\end{enumerate}

\vspace{1em}
\subsubsection{Performance Requirements}

\textbf{NFRTC7}\\
\textbf{Title:} Image Generation Speed Test\\
\textbf{Control:} Automatic\\
\textbf{Initial State:} The system is running in its production environment.\\
\textbf{Input:} A request to generate a synthetic chest X-ray image.\\
\textbf{Output:} The image is rendered in the interface within \verb|LATENCYTHRESHOLD| of request submission.\\
\textbf{Test Case Derivation:} Quick output ensures user efficiency and supports timely review during clinical workflows.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Initiate an image generation request.
  \item Log start and end timestamps.
  \item Compute total latency.
  \item Confirm it does not exceed \verb|LATENCYTHRESHOLD|.
  \item Repeat 20 times and average.
\end{enumerate}

\vspace{1em}

\textbf{NFRTC8}\\
\textbf{Title:} System Stability and Uptime Monitoring\\
\textbf{Control:} Automatic (Continuous)\\
\textbf{Initial State:} The system is deployed and uptime tracking enabled.\\
\textbf{Input:} Continuous automated health checks.\\
\textbf{Output:} Uptime exceeds \verb|UPTIMETHRESHOLD| with no more than \verb|MAXDOWNTIME| per week.\\
\textbf{Test Case Derivation:} System must remain online reliably to support consistent user access.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Ping the system every 5 minutes for 30 days.
  \item Log successes and failures.
  \item Calculate uptime and flag excessive downtime.
\end{enumerate}

\vspace{1em}

\textbf{NFRTC9}\\
\textbf{Title:} Single Image Generation Capacity Enforcement\\
\textbf{Control:} Manual and Automatic\\
\textbf{Initial State:} The system is processing an image generation request.\\
\textbf{Input:} Two image generation requests submitted concurrently.\\
\textbf{Output:} The second request is rejected with a message: "Only one generation job allowed at a time."\\
\textbf{Test Case Derivation:} Concurrency limits preserve backend stability and ensure fairness in usage.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Submit Request A and wait.
  \item Submit Request B before A completes.
  \item Verify B is rejected with a clear message.
  \item Ensure A completes successfully.
\end{enumerate}

\vspace{1em}
\subsubsection{Security Requirements}

\textbf{NFRTC10}\\
\textbf{Title:} Public Accessibility Without Authentication\\
\textbf{Control:} Manual\\
\textbf{Initial State:} The web application is deployed publicly.\\
\textbf{Input:} Access the app from multiple devices/IPs without logging in.\\
\textbf{Output:} Core features remain accessible without login prompts.\\
\textbf{Test Case Derivation:} Unauthenticated access supports quick demo/testing and avoids blocking users behind credentials.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Open the app in incognito mode on multiple devices.
  \item Try all core features without logging in.
  \item Confirm consistent, unrestricted access.
\end{enumerate}

\vspace{1em}

\textbf{NFRTC11}\\
\textbf{Title:} Verification of Non-Collection of Personal Data\\
\textbf{Control:} Manual and Automatic\\
\textbf{Initial State:} Application is live and traffic can be monitored.\\
\textbf{Input:} User actions across sessions.\\
\textbf{Output:} No personally identifiable information (PII) is collected or stored.\\
\textbf{Test Case Derivation:} No collection of PII ensures compliance with privacy regulations and avoids legal risk.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Perform user interactions and monitor requests.
  \item Check network traffic for PII.
  \item Verify logs, local storage, and databases for any user-specific data.
\end{enumerate}

\vspace{1em}
\subsubsection{Supportability Requirements}

\textbf{NFRTC12}\\
\textbf{Title:} Availability and Sufficiency of User Documentation\\
\textbf{Control:} Manual\\
\textbf{Initial State:} Application includes user-facing documentation.\\
\textbf{Input:} Users attempt tasks using only documentation.\\
\textbf{Output:} At least 80\% complete core workflows without extra support.\\
\textbf{Test Case Derivation:} Complete, clear documentation enables users to operate the system independently.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Provide only the documentation to test users.
  \item Ask them to complete image generation and viewing.
  \item Track completion rates and clarity feedback.
\end{enumerate}

\vspace{1em}
\subsubsection{Cultural Requirements}

\textbf{NFRTC13}\\
\textbf{Title:} Functionality of Browser-Based Language Translation\\
\textbf{Control:} Manual\\
\textbf{Initial State:} App UI is in English and live.\\
\textbf{Input:} Use browser translation to switch to other languages.\\
\textbf{Output:} UI renders properly and all features remain functional.\\
\textbf{Test Case Derivation:} Translation compatibility ensures global usability and avoids UI issues during localization.\\
\textbf{How the test will be performed:}
\begin{enumerate}
  \item Open the app in Chrome or Edge.
  \item Activate built-in translation and switch languages.
  \item Verify UI updates and core functions work.
  \item Repeat for at least 3 languages.
\end{enumerate}


\subsection{Untested Non-Functional Requirements}

% Feedback addressed:
% - Clearly justified omissions with scope/timeline constraints
% - Requirements explicitly referenced by label
% - Reformatted for clarity and readability

Some non-functional requirements are acknowledged but excluded from validation due to current project constraints. These constraints include lack of long-term deployment, restricted access to infrastructure, and project scope boundaries. The following are recognized but untested:

\begin{itemize}
  \item \textbf{NF-SER0 (Scalability or Extensibility Requirements):} 
  Testing for horizontal/vertical scaling, modularity, or plugin compatibility is outside current implementation scope.
  
  \item \textbf{NF-LR0 (Longevity Requirements):} 
  Validating performance and reliability over a three-year timeline is infeasible within this capstone cycle.
  
  \item \textbf{NF-MR0 (Maintenance Requirements):} 
  Assessing ease of future updates, bug fixes, and dependency upgrades requires an operational phase post-deployment.
  
  \item \textbf{NF-EPE0 (Expected Physical Environment):} 
  The software is environment-agnostic and runs entirely within a digital infrastructure. No real-world physical testing required.
  
  \item \textbf{NF-RIAS0 (Interfacing with Adjacent Systems):} 
  No external system integration exists in the current design. Therefore, interfacing behavior falls outside of scope.
  
  \item \textbf{NF-IR0 (Integrity Requirements):} 
  The system outputs anonymized visual data only. Since no patient identifiers or modifiable inputs are used, specific data integrity testing was deemed unnecessary.
  
  \item \textbf{NF-AR0 (Audit Requirements):} 
  While security is enforced via encryption and role-based access, no formal third-party audit or audit trail requirement was defined for this phase.
\end{itemize}
\subsection{Traceability Between Test Cases and Requirements}

% Feedback addressed:
% - Updated based on revised SRS (2025)
% - Replaced outdated diffusion model references with CNN-based classification (FR3, FR4)
% - Added traceability to new requirements (e.g., FR12 low-confidence alerts)
% - Retained consistent VnV plan format

\begin{table}[h]
  \centering
  \begin{tabular}{|p{0.7\textwidth}|p{0.23\textwidth}|}
    \hline
    \textbf{Functional Requirement} & \textbf{Test Cases} \\
    \hline
    FR1: Upload supported chest X-ray images & FRTC1 \\
    \hline
    FR2: Preprocess images (resize, normalize, grayscale) & FRTC2 \\
    \hline
    FR3: CNN classification with confidence scores & FRTC3 \\
    \hline
    FR4: Multi-disease classification per image & FRTC5 \\
    \hline
    FR5: Display predicted labels, confidence scores, and heatmaps & FRTC6 \\
    \hline
    FR6: Downloadable diagnostic report with visuals & FRTC4 \\
    \hline
    FR7: Role-based access control & FRTC7, FRTC8 \\
    \hline
    FR8: Secure API endpoint for classification & FRTC11 \\
    \hline
    FR9: Handle invalid uploads with error messaging & FRTC9 \\
    \hline
    FR10: Dashboard view of predictions with timestamps & FRTC10 \\
    \hline
  \end{tabular}
  \caption{Traceability Matrix - Functional Requirements to System Test Cases}
\end{table}
\pagebreak


\subsection{Traceability Between Test Cases and Requirements}

\begin{table}[H]
  \centering
  \caption{Traceability Between Non-Functional Requirements and Test Cases}
  \renewcommand{\arraystretch}{1.3}
  \begin{tabularx}{\textwidth}{|>{\raggedright\arraybackslash}X|>{\raggedright\arraybackslash}m{0.23\textwidth}|}
    \hline
    \textbf{Non-Functional Requirement} & \textbf{Test Cases} \\
    \hline
    NF-AR1: Calming and professional color scheme & NFRTC1 \\
    \hline
    NF-SR1: Minimalistic and functional design & NFRTC2 \\
    \hline
    NF-SR2: Clean, organized layout & NFRTC2 \\
    \hline
    NF-EUR0: Efficient task completion & NFRTC3 \\
    \hline
    NF-EU2: Intuitive post-training use & NFRTC4 \\
    \hline
    NF-LR0: Learnability and uptime expectations & NFRTC5, NFRTC8 \\
    \hline
    NF-UPR0: Realism of synthetic images & NFRTC6 \\
    \hline
    NF-SLR0: Latency under 60 seconds & NFRTC7 \\
    \hline
    NF-RFTR0: Fault tolerance for corrupted uploads & FRTC9 \\
    \hline
    NF-CR0: One-at-a-time image generation capacity & NFRTC9 \\
    \hline
    NF-CR1: Cultural and language support via translation & NFRTC13 \\
    \hline
    NF-PR0: No personal data collection & NFRTC11 \\
    \hline
    NF-IR0: Access limited to authenticated sessions & FRTC7, NFRTC11 \\
    \hline
    NF-AUR0: Logs of access and configuration & FRTC8 \\
    \hline
    NF-RIAS0: Integration with PACS/EHR & \textit{Untested in this version} \\
    \hline
    NF-REL0: MVP feature delivery & FRTC1, FRTC3, FRTC4 \\
    \hline
    NF-REL1: Logging and API configuration options & FRTC11 \\
    \hline
    NF-MR0: Modular architecture & \textit{Documented, not tested} \\
    \hline
    NF-MR1: Git-based version control & \textit{Documented, not tested} \\
    \hline
    NF-SCR0: Compliance with HL7/FHIR/DICOM & \textit{Reviewed, not tested} \\
    \hline
    NF-SER0: Scalability and modularity & \textit{Untested in this version} \\
    \hline
  \end{tabularx}
\end{table}



% ----------------------------- Symbolic Constants -----------------------------
\newcommand{\SUCCESSRATE}{80\%}
\newcommand{\CONFIDENCERATING}{4}
% -------------------------------------------------------------------------------


% Reviewer Feedback Incorporated:
% - Avoided magic numbers (used D for number of diseases)
% - Enhanced traceability and specificity
% - Used inline documentation-style comments for maintainability
% [Feedback] Be sure to use symbolic constants instead of magic numbers like 90\%.
% [Change] For unit tests that evaluate accuracy thresholds, constants such as \texttt{\ACCURACYTHRESHOLD{}} are defined within the testing configuration.

% [Feedback] Traceability to Requirements.
% [Change] All unit test mappings will be documented inline using symbolic constants and meaningful test names. Traceability tables for each module-to-requirement mapping appear in Section 4.4.

% [Feedback] Unit tests are not required yet since the design is still pending.
% [Change] This section outlines the unit test scope conceptually so that tests can be reused and implemented post-design without major revision.

\section{Unit Test Description}

This section outlines the approach and rationale for the unit tests designed to verify each module within the chest X-ray diagnostic application. The overall philosophy for test case selection centers on ensuring each module functions correctly in isolation, covering both standard functionality and potential edge cases. This approach provides a robust foundation to identify and address any issues at the unit level before integrating modules into the broader system.

To streamline testing and avoid redundancy, each module's tests are designed to validate both typical and boundary conditions. This strategy includes creating a baseline test case for normal expected behavior, accompanied by additional edge cases to capture unexpected or extreme inputs. This allows for comprehensive testing without unnecessary detail for every possible scenario.

For efficiency, test cases prioritize high-risk functions and core functionalities. Tests are organized to:
\begin{itemize}
  \item Validate standard functionality with a single test for typical cases.
  \item Test critical edge cases that reflect likely and realistic error conditions (e.g., handling of empty data, invalid formats, and unexpected large data inputs).
  \item Ensure modules perform within expected limits for nonfunctional requirements, such as performance and security, through specialized tests.
  \item Include module-specific performance tracking across variable dataset conditions to ensure stability and reliability.
  \item Evaluate modular interactions such as dataset loading, model persistence, and API response validation.
\end{itemize}

Where possible, detailed input/output descriptions are omitted in favor of referring to well-documented unit testing code, which maintains readable and meaningful test names. This approach not only saves space but also facilitates future maintenance and expansion, as new edge cases can be added to the codebase directly.

\subsection{Unit Testing Scope}

Unit testing for this project targets the core components of the chest X-ray diagnostic pipeline, ensuring correctness, resilience, and modularity of each individual module. The scope explicitly addresses all critical stages from data ingestion and transformation to model training and persistence, aligned with the architecture described in the updated SRS and design documents.

Each module is tested in isolation under both standard and edge-case conditions. These unit tests are critical for identifying logic flaws, integration risks, and regressions early in the development lifecycle. By verifying core functionalities before system-wide testing, we ensure each module behaves predictably and performs as expected across varied dataset conditions, such as missing files, malformed inputs, and irregular class distributions.

Unit tests span the following core components:
\begin{itemize}
  \item \textbf{DataPreparation Module} - Handles transformation of raw dataset metadata into structured model inputs. Tests verify:
    \begin{itemize}
      \item Label binarization and class alignment (e.g., \texttt{test\_binarize\_labels}).
      \item Stratified data splitting and sample preservation (\texttt{test\_split\_data}).
      \item Transformation pipeline integrity for data augmentation and normalization.
    \end{itemize}

  \item \textbf{DataRetrieval Module} - Automates NIH dataset download and extraction. Unit tests confirm:
    \begin{itemize}
      \item Archive integrity and target directory structure (\texttt{test\_extract\_dataset}).
      \item Network download stability and file caching (\texttt{test\_download\_dataset}).
    \end{itemize}

  \item \textbf{ModelArchitecture Module} - Encapsulates CNN layer definitions and model assembly:
    \begin{itemize}
      \item Validation of model structure and final output layer shape.
      \item Confirmation of feature extractor freezing behavior.
      \item Forward pass shape and range correctness.
    \end{itemize}

  \item \textbf{Training Module} - Manages training loop, early stopping, and model saving:
    \begin{itemize}
      \item Training loop correctness and early error detection (\texttt{test\_train\_step}).
      \item Prediction output distribution and confidence scoring (\texttt{test\_prediction}).
    \end{itemize}

  \item \textbf{MLBackend Module} - Provides inference API and model loading:
    \begin{itemize}
      \item Connectivity test and response latency (\texttt{test\_connection}).
      \item Inference error handling and JSON format validation.
    \end{itemize}

  \item \textbf{ModelInterface Module} - UI-triggered logic for uploads, predictions, and report export:
    \begin{itemize}
      \item Verification of image format validation and preview rendering.
      \item Confirmation that prediction and confidence outputs are rendered correctly.
      \item Robustness of report generation and download functionality.
    \end{itemize}

  \item \textbf{AuthClient and Authorization Modules} - Implements frontend and backend login logic:
    \begin{itemize}
      \item Token creation and validation for protected API routes.
      \item Error handling for failed logins and permission mismatches.
    \end{itemize}

  \item \textbf{Config Module} - Backend boot configuration including JWT and CORS:
    \begin{itemize}
      \item Test security filter chain setup.
      \item Validate backend-to-backend communication setup.
    \end{itemize}
\end{itemize}

Modules not included in this unit testing scope:
\begin{itemize}
  \item \textbf{Third-party libraries or frameworks} (e.g., Flask, Spring Security, PyTorch): These are assumed to be externally validated and are only tested through integration points.
  \item \textbf{Static assets} such as UI CSS or icon libraries, which do not affect logic.
\end{itemize}



\subsection{Tests for Functional Requirements}

\section{Unit Test Description}
\label{sec:unit-tests}

\subsection{Unit Testing Scope}
All internal modules developed by the team are within the scope of unit testing. External libraries (e.g., PyTorch, Albumentations, JWT libraries) are considered out of scope and tested via integration. Priority is given to \texttt{ModelInterface}, \texttt{DataPreparation}, \texttt{MLBackend}, and \texttt{Training} due to their centrality in image processing and prediction.

\subsection{Tests for Functional Requirements}

\subsubsection{ModelInterface (M1)}
This module is tested via frontend test suites (React + Jest). Tests cover input validation, backend interaction, and PDF generation logic.

\textbf{$1$. MI-test-upload-valid}\\
Type: Automatic\\
Initial State: No image uploaded\\
Input: Valid image file (e.g., .jpg)\\
Output: Image preview renders in UI\\
Test Case Derivation: Valid images should trigger preview render for verification.\\
How test will be performed: Simulate file input and assert image tag is inserted into DOM.

\textbf{$2$. MI-test-analyze-error}\\
Type: Automatic\\
Initial State: Image uploaded\\
Input: API call fails (mock $500$ error)\\
Output: Error message appears to user\\
Test Case Derivation: Server-side issues should produce user-readable feedback.\\
How test will be performed: Mock failed request and confirm displayed error.

\textbf{$3$. MI-test-report-download}\\
Type: Automatic\\
Initial State: Analysis completed and UI visible\\
Input: Click on "Download Report"\\
Output: File download initiated\\
Test Case Derivation: Download trigger must call blob creation and trigger anchor link.\\
How test will be performed: Mock browser APIs and confirm link click occurs.

\subsubsection{AuthClient (M2)}
Frontend module tested using UI simulation and API mocking. Test cases ensure proper error handling and form state behavior.

\textbf{$1$. AC-test-register-valid}\\
Type: Automatic\\
Initial State: Registration form shown\\
Input: Valid username, email, and password\\
Output: Success message and redirect to login\\
Test Case Derivation: Valid input should create account and guide user to next step.\\
How test will be performed: Simulate form fill and submit; assert redirect and toast.

\textbf{$2$. AC-test-login-invalid}\\
Type: Automatic\\
Initial State: Login form active\\
Input: Invalid password for known user\\
Output: Authentication error shown\\
Test Case Derivation: Failed login must provide helpful error without crashing.\\
How test will be performed: Submit credentials and verify error state.

\subsubsection{DataRetrieval (M3)}
Focuses on verifying NIH image archive download and extraction operations.

\textbf{$1$. DR-test-download-skipped-if-exists}\\
Type: Automatic\\
Initial State: Partial image set exists\\
Input: prepareData() call\\
Output: Logs skipping already-downloaded files\\
Test Case Derivation: Should avoid re-downloading existing content.\\
How test will be performed: Pre-create dummy tar.gz and assert skip path.

\textbf{$2$. DR-test-extraction-failure}\\
Type: Automatic\\
Initial State: Corrupt archive present\\
Input: prepareData() call\\
Output: Raises FileIOError\\
Test Case Derivation: Corrupted archives must halt cleanly and raise traceable error.\\
How test will be performed: Replace archive with dummy data and invoke.

\subsubsection{DataPreparation (M4)}
This is one of the most testable components. Tests span label parsing, stratified splitting, and augmentation.

\textbf{$1$. DP-test-preprocess-valid}\\
Type: Automatic\\
Initial State: Raw CSV loaded\\
Input: DataFrame with label values\\
Output: Filtered rows with valid disease strings\\
Test Case Derivation: Ensures rows with invalid/no labels are removed.\\
How test will be performed: Feed test CSV and verify rows retained.

\textbf{$2$. DP-test-binarize-labels}\\
Type: Automatic\\
Initial State: DataFrame contains pipe-separated label column\\
Input: e.g., "Pneumonia|Infiltration"\\
Output: Columns pneumonia=$1$, infiltration=$1$\\
Test Case Derivation: Multi-label parsing must yield one-hot vector.\\
How test will be performed: Validate output columns after run.

\textbf{$3$. DP-test-stratified-split}\\
Type: Automatic\\
Initial State: DataFrame has labels\\
Input: Call splitData()\\
Output: $80$/$20$ split with class distribution retained\\
Test Case Derivation: Split should preserve label proportion across sets.\\
How test will be performed: Compare $value_counts$ across sets.

\textbf{$4$. DP-test-transform}\\
Type: Automatic\\
Initial State: Transform pipeline defined\\
Input: Image tensor\\
Output: Transformed tensor of correct shape\\
Test Case Derivation: Valid transformations must preserve shape and normalization.\\
How test will be performed: Pass dummy image and inspect result.

\textbf{$5$. DP-test-training-stats}\\
Type: Automatic\\
Initial State: Training dataset loaded\\
Input: binarized label matrix\\
Output: posWeights and avgPos count\\
Test Case Derivation: Loss balancing terms must reflect true label sparsity.\\
How test will be performed: Manually compute and compare.

\subsubsection{Authorization (M5)}
Covers backend login, token issuance, and registration flows.

\textbf{$1$. AUTH-test-token-generation}\\
Type: Automatic\\
Initial State: Valid user credentials\\
Input: Authenticated session\\
Output: JWT returned\\
Test Case Derivation: Login should result in signed token creation.\\
How test will be performed: Use mock auth and assert token schema.

\textbf{$2$. AUTH-test-invalid-token}\\
Type: Automatic\\
Initial State: Token from expired session\\
Input: validateToken() call\\
Output: False or exception raised\\
Test Case Derivation: Expired/malformed tokens should not validate.\\
How test will be performed: Manually craft bad token.

\subsubsection{MLBackend (M6)}
Python Flask server responsible for model inference.

\textbf{$1$. MLB-test-healthcheck}\\
Type: Automatic\\
Initial State: Backend server running\\
Input: GET /test\\
Output: $200$ OK + "API connected"\\
Test Case Derivation: Reachability check verifies backend is online.\\
How test will be performed: Use test client to make GET call.

\textbf{$2$. MLB-test-valid-prediction}\\
Type: Automatic\\
Initial State: Model loaded\\
Input: Valid chest X-ray image\\
Output: JSON with predictions\\
Test Case Derivation: API must return class-wise probabilities.\\
How test will be performed: Use file upload with sample image and parse output.

\textbf{$3$. MLB-test-missing-file}\\
Type: Automatic\\
Initial State: POST /predict triggered without image\\
Input: No file\\
Output: HTTP $400$ with error\\
Test Case Derivation: Missing files must be caught and responded to.\\
How test will be performed: Empty POST request.

\subsubsection{ModelArchitecture (M7)}

\textbf{$1$. MA-test-model-construction}\\
Type: Automatic\\
Initial State: None\\
Input: Call assembleFullModel($13$)\\
Output: nn.Module with $13$ output units\\
Test Case Derivation: Classification head must output correct dimensions.\\
How test will be performed: Create model and inspect final layer.

\textbf{$2$. MA-test-layer-freeze}\\
Type: Automatic\\
Initial State: Base model instantiated\\
Input: freezeBaseLayers()\\
Output: Base layers require\_grad = False\\
Test Case Derivation: Prevent gradient updates in transfer learning.\\
How test will be performed: Loop through layer flags.

\subsubsection{Training (M8)}

\textbf{$1$. TRAIN-test-single-batch}\\
Type: Automatic\\
Initial State: Model + loader initialized\\
Input: One training batch\\
Output: Gradient computed\\
Test Case Derivation: Confirms training loop runs per-batch loss\\
How test will be performed: Forward, loss, backward.

\textbf{$2$. TRAIN-test-eval-shape}\\
Type: Automatic\\
Initial State: Model in eval mode\\
Input: Batch of images\\
Output: Tensor (B, $13$) with logits\\
Test Case Derivation: Output should be interpretable for all labels.\\
How test will be performed: Dummy input, inspect shape.

\textbf{$3$. TRAIN-test-margin-loss}\\
Type: Automatic\\
Initial State: Tensors prepared\\
Input: logits, labels\\
Output: Scalar margin loss\\
Test Case Derivation: Checks added regularization term.\\
How test will be performed: Call loss function on fixed input.




\subsection{Tests for Nonfunctional Requirements}

\subsubsection{MLBackend (M6)}

\textbf{$1$. NF-M6-bulk-prediction-timing}\\
Type: Automatic\\
Initial State: ML model loaded with GPU enabled\\
Input: Batch of $32$ chest X-rays\\
Output: Inference time under $10$ seconds\\
Test Case Derivation: Ensures latency is acceptable for real-time use.\\
How test will be performed: Log start and end time of batch inference.

\textbf{$2$. NF-M6-no-crash-on-noisy-data}\\
Type: Automatic\\
Initial State: Model ready for prediction\\
Input: Corrupted or low-quality images\\
Output: Prediction result or warning, no crash\\
Test Case Derivation: Validates fault-tolerance to suboptimal inputs.\\
How test will be performed: Feed known noisy inputs and observe log/output.

\subsubsection{ModelArchitecture (M7)}

\textbf{$1$. NF-M7-heatmap-integrity}\\
Type: Automatic (Visual and IOU score)\\
Initial State: Trained model with intermediate layers accessible\\
Input: Image and expected anatomical region\\
Output: Heatmap correctly focuses relevant areas\\
Test Case Derivation: Verifies interpretability of model through Grad-CAM.\\
How test will be performed: Compare heatmap overlay with expected lung region.

\subsubsection{Training (M8)}

\textbf{$1$. NF-M8-bias-evaluation}\\
Type: Automatic\\
Initial State: Model trained with metadata access\\
Input: Dataset with demographic splits\\
Output: No major metric disparity across subgroups\\
Test Case Derivation: Checks demographic fairness of model behavior.\\
How test will be performed: Evaluate recall/specificity per subgroup.

\subsubsection{Authorization (M5)}

\textbf{$1$. NF-M5-invalid-token-access}\\
Type: Automatic\\
Initial State: Endpoint secured by JWT middleware\\
Input: Request with malformed/expired token\\
Output: $403$ Forbidden with log entry\\
Test Case Derivation: Enforces secure access and auditability.\\
How test will be performed: Simulate invalid request and verify log/status.

\subsubsection{Config (M6)}

\textbf{$1$. NF-M6-api-connectivity-test}\\
Type: Automatic\\
Initial State: Config boot logic active\\
Input: Attempt to ping Flask backend\\
Output: Successful response from ML backend\\
Test Case Derivation: Ensures connection across stack is reliable.\\
How test will be performed: Run connectivity ping on init and assert success.




subsection{Traceability Between Test Cases and Modules}

\begin{table}[h!]
  \centering
  \caption{Legend for Module Labels}
  \begin{tabular}{|c|p{0.75\textwidth}|}
    \hline
    \textbf{Module Label} & \textbf{Description} \\
    \hline
    M1 & DataPreprocessing - Cleans metadata, binarizes labels, applies transforms, and performs stratified splits. \\
    \hline
    M2 & ChestXrayDataset - Custom PyTorch dataset for loading X-ray images and binary labels. \\
    \hline
    M3 & ModelArchitecture - Builds MobileNetV2-based CNN and classification head for disease prediction. \\
    \hline
    M4 & Training - Manages training loop, loss calculation, and metric evaluation. \\
    \hline
    M5 & ModelPersistence - Handles saving and loading model checkpoints. \\
    \hline
    M6 & DataRetrieval - Downloads and extracts NIH datasets from public URLs. \\
    \hline
    M7 & MLBackend - Flask backend for API communication and image inference. \\
    \hline
    M8 & Config - Spring Boot config logic (security, JWT, REST communication). \\
    \hline
    M9 & Authorization - Auth/login/signup logic for protected user access control. \\
    \hline
  \end{tabular}
\end{table}

\begin{table}[h!]
  \centering
  \caption{Traceability Matrix for Functional Requirement Test Cases (FRTC)}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{FRTC} & \textbf{M1} & \textbf{M2} & \textbf{M3} & \textbf{M4} & \textbf{M5} & \textbf{M6} & \textbf{M7} & \textbf{M8} & \textbf{M9} \\
    \hline
    FRTC1 & X & X &   &   &   &   &   &   &   \\
    \hline
    FRTC2 & X &   &   &   &   &   & X &   &   \\
    \hline
    FRTC3 & X & X &   &   &   &   & X &   &   \\
    \hline
    FRTC4 &   &   & X & X &   &   & X &   &   \\
    \hline
    FRTC5 &   &   & X & X &   &   & X &   &   \\
    \hline
    FRTC6 &   &   &   &   &   &   & X &   &   \\
    \hline
    FRTC7 &   &   &   &   &   &   &   &   & X \\
    \hline
    FRTC8 &   &   &   &   &   &   &   &   & X \\
    \hline
    FRTC9 &   &   &   &   &   &   &   & X & X \\
    \hline
    FRTC10 &   &   &   &   &   &   &   & X & X \\
    \hline
  \end{tabular}
\end{table}


\begin{table}[h!]
  \centering
  \caption{Traceability Matrix for Non-Functional Requirement Test Cases (NFRTC)}
  \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
    \hline
    \textbf{NFRTC} & \textbf{M1} & \textbf{M2} & \textbf{M3} & \textbf{M4} & \textbf{M5} & \textbf{M6} & \textbf{M7} & \textbf{M8} & \textbf{M9} \\
    \hline
    NFRTC1  &   &   &   &   &   &   &   & X &   \\ % UI appearance (frontend)
    \hline
    NFRTC2  &   &   &   &   &   &   &   & X &   \\ % UI layout and visual consistency
    \hline
    NFRTC3  &   &   &   &   &   &   &   & X &   \\ % Ease of use
    \hline
    NFRTC4  &   &   &   &   &   &   &   & X &   \\ % Memorability of flow
    \hline
    NFRTC5  & X & X &   &   &   &   &   & X &   \\ % Learning curve - uploading and running
    \hline
    NFRTC6  &   &   & X & X &   &   &   &   &   \\ % Realism of generated images
    \hline
    NFRTC7  &   &   &   &   &   &   & X &   &   \\ % Image generation speed test
    \hline
    NFRTC8  &   &   &   &   &   &   & X &   &   \\ % System uptime
    \hline
    NFRTC9  &   &   &   &   &   &   & X &   &   \\ % Concurrency enforcement
    \hline
    NFRTC10 &   &   &   &   &   &   &   & X &   \\ % No-login access
    \hline
    NFRTC11 &   &   &   &   &   &   & X &   &   \\ % PII non-collection
    \hline
    NFRTC12 &   &   &   &   &   &   &   & X &   \\ % Documentation-only navigation
    \hline
    NFRTC13 &   &   &   &   &   &   &   & X &   \\ % Language translation
    \hline
  \end{tabular}
\end{table}


\pagebreak
\newpage

\section{Appendix}
In this section, additional information that complements the V\&V Plan is included.


\subsection{Symbolic Parameters}

The following symbolic parameters are used throughout the system and unit test cases to avoid hard-coded values. This approach improves clarity, consistency, and maintainability.

\begin{table}[H]
\centering
\begin{tabular}{|l|p{11cm}|}
\hline
\textbf{Symbolic Name} & \textbf{Definition or Description} \\
\hline
\texttt{MIN\_CLASSIFICATION\_ACCURACY} & Minimum per-class accuracy required for CNN-based disease prediction (previously 60\%). \\
\texttt{MIN\_MULTILABEL\_PERFORMANCE} & Minimum precision and recall required for multi-label classification tasks (previously ACCURACY\_THRESHOLD). \\
\texttt{DISPLAY\_LATENCY\_THRESHOLD} & Maximum allowed time to display predictions and heatmaps on the UI (previously MAX\_RENDER\_TIME). \\
\texttt{MIN\_VALIDATION\_ACCURACY} & Required accuracy across disease classes on validation datasets (previously 90\%). \\
\texttt{MAX\_BATCH\_PREDICTION\_LATENCY} & Maximum duration for batch predictions over multiple X-rays (previously under 10 seconds). \\
\texttt{MAX\_USER\_DETECTION\_ACCURACY} & Maximum rate users can identify synthetic vs. real X-rays (used in realism test, previously 60\%). \\
\texttt{USER\_SUCCESS\_THRESHOLD} & Minimum user success rate on tasks or UI usability favorability (previously at least 80\%). \\
\hline
\end{tabular}
\caption{Symbolic parameters used in V\&V test definitions.}
\end{table}

The definition of test cases will call for certain symbolic constants. Their values are defined in this section for easy maintenance. See the following table for reference.

\begin{table}[h]
  \centering
  \begin{tabular}{|p{0.3\textwidth}|p{0.7\textwidth}|}
    \hline
    \multicolumn{1}{|c|}{\textbf{Symbolic Name}} & \multicolumn{1}{c|}{\textbf{Definition or Description}} \\ \hline
    DATASET-NAME & The chest X-ray imaging datasets used for development and testing. Details are as follows: \\ \hline
    MIMIC-CXR & A standard labeled chest X-ray dataset commonly used for medical imaging tasks. \\ \hline
    Chest ImaGenome & An extension of MIMIC-CXR, with detailed annotations including 29 bounding boxes on anatomical regions. This dataset links textual report segments to specific bounding boxes, supporting tasks in segmentation and disease localization. \\ \hline
  \end{tabular}
\end{table}

\subsection{Usability Survey Questions?}

The following survey should be filled out by users after using the system for 5 to 15 minutes. This feedback helps evaluate the user experience and identifies areas for improvement.

\subsubsection{User Experience Survey}

\textbf{Time using system:}
Please provide a rating between 0 and 10 for each of the following categories, with 0 being very difficult or undesirable, and 10 being very easy or desirable.
\begin{enumerate}
  \item Ease of Use: 0 1 2 3 4 5 6 7 8 9 10 \\
    (0 = very difficult to use, 10 = very easy to use)
  \item Navigation: 0 1 2 3 4 5 6 7 8 9 10 \\
    (0 = hard to find what you're looking for, 10 = easy to find what you're looking for)
  \item Readability: 0 1 2 3 4 5 6 7 8 9 10 \\
    (0 = hard to understand information, 10 = very easy to understand)
  \item Look and Feel: 0 1 2 3 4 5 6 7 8 9 10 \\
    (0 = visually unappealing, 10 = visually appealing)
\end{enumerate}

\subsubsection{Additional Feedback:}
Notes: (Space for users to leave any additional comments or suggestions about the system.)

\newpage{}
\section*{Appendix --- Reflection}

\textbf{Current Content:} Summarizes lessons learned and challenges. \\
\textbf{\textcolor{blue}{Update Needed:}} Expand reflections to cover \textbf{challenges in classification generalization, optimizing hyperparameters, and improving model transparency for clinical adoption}.


\subsection{Question 1}
What went well while writing this deliverable?

\textbf{Harrison Chiu:}  Planning out the modules for functional testing went pretty smoothly. Reviewing SOLID principles helped a lot and I was able to structure each module with a clear purpose, which made creating the tests feel straightforward. I even tried some initial tests on my own machine to catch edge cases early on, and it gave me confidence that I was covering all the important scenarios. That fuzz testing experiment helped me feel like I was on the right track.

\textbf{Hamza Issa:} Working on identifying tests for functional requirements felt really intuitive. After we reached a consensus regarding the functional requirements and our ability to implement it, it was quite simple to devise tests that were complete and meaningful, as part of creating functional requirements is that the expectation action or output is typically discrete.

\textbf{Gurnoor Bal:} Working on the SRS verification was actually pretty satisfying because I got to make sure everything was crystal clear. I found a few areas in the SRS that needed improvement, and being able to fix those early felt good. Plus, setting up the traceability matrix was helpful since it kept things organized and made it easy to link each requirement with its specific test case. It felt like things were clicking into place.

\textbf{Jared Paul:} I liked working on the design verification part because it let me focus on making sure the system was organized and easy to work with. I emphasized modularity and made sure each module was self-contained, which will make the system easier to maintain down the road. I also created a checklist to make sure each module followed solid design practices, so I felt like I was covering the important aspects without missing anything.

\textbf{Ahmad Hamadi:} Writing the validation plan was rewarding because I got to think about how people would actually use the tool, especially radiologists. I set up task-based testing and feedback sessions, so we'll get a real sense of how practical the tool is. Using actual datasets to check the model's accuracy felt like a solid choice too. It felt like I was creating a plan that would make sure the tool was both accurate and user-friendly.

\subsection{Question 2}
What pain points did you experience during this deliverable, and how did you resolve them? \\

\textbf{Hamza Issa:} I found that in order to create a V\&V that was really complete meant that I needed to not just need a basic abstract understanding of the theory around Diffusion models, rather I needed to actually understand the theory, how it is typically implemented today in research and industry, so much so that I needed to find out the type of data structure to expect as output which would be a rather complex matrix. This entire process of learning was really challenging as I was a novice in the subject. But I managed to solve this by purchasing a course on Udemy regarding Theory and Implementation of Diffusion models which helped me really better understand and write a V\&V plan that actually made sense in the context of diffusion models.

\textbf{Harrison Chiu:} A bit of a challenge was figuring out just how detailed each test case needed to be. I kept wondering if I was getting too specific or not specific enough. To fix this, I made a quick checklist of essential edge cases based on the project requirements and ran it by the team. Getting their input reassured me that I was covering the right bases without going overboard.

\textbf{Gurnoor Bal:} The main problem was with some of the requirements that were a bit too general or vague. It was hard to nail down exactly what needed verifying. I decided to check in with our project supervisor to get a better handle on what was expected, and I left a few notes in the document to flag areas we might need to clarify later. That way, the process stayed clear without me having to guess on details.

\textbf{Jared Paul:} One thing that wasn't easy was verifying the interfaces for each module. Some of the connections needed more detail to line up with the rest of the design. To sort this out, I went back and added specific inputs and outputs for each module, which made sure everything matched up well with the Module Guide. It took a bit more time, but in the end, it made the design much clearer.

\textbf{Ahmad Hamadi:} The tricky part was making sure the tests weren't just covering easy scenarios. I wanted to be sure they were meaningful. I ran into a few issues with environment consistency, which caused random test failures. To fix this, I standardized the setup and added a few retries for tests that occasionally flaked. Then I went back to the requirements to make sure the tests were focused on the important functions. This way, the tests ended up being both reliable and relevant.

\subsection{Question 3}
What knowledge and skills will the team collectively need to acquire to
successfully complete the verification and validation of your project?
Examples of possible knowledge and skills include dynamic testing knowledge,
static testing knowledge, specific tool usage, Valgrind etc.  You should look to
identify at least one item for each team member.\\

Several different and diverse skills are required in order to create the verification and validation plan for this Chest X-ray Diffusion Model Research project.

\textbf{Software Specification Design}\\
Our team will need a strong understanding of software specification design to grasp the project context and outline the required functions and modules. By defining each function's purpose, inputs, and edge cases, we can plan the implementation of functions accurately according to the specifications. This design phase will serve as the foundation for later development stages.

\textbf{Unit Test Planning}\\
Developing robust unit tests will be crucial. Our team must create comprehensive tests that ensure each function achieves its intended purpose reliably. This will require us to account for all relevant edge cases to avoid misleading results and ensure the accuracy and completeness of our unit tests as a core skill for our V\&V plan.

\textbf{Software Design Principles}\\
To maintain a flexible and maintainable codebase, we'll need to prioritize software design principles, particularly the SOLID principles. For example, we plan to structure modules to follow the Single Responsibility Principle, which will help ensure clarity and cohesion in the design, allowing future engineers to work with the code more effectively.

\textbf{Diffusion Model Concepts and Implementation}\\
Understanding both the theoretical and practical aspects of diffusion models will be essential for designing an accurate V\&V plan. Our team will need knowledge of the inputs, expected outputs, and common data structures used in similar applications to develop thorough and effective tests.

\subsection{Question 4}
For each of the knowledge areas and skills identified in the previous
question, what are at least two approaches to acquiring the knowledge or
mastering the skill?  Of the identified approaches, which will each team
member pursue, and why did they make this choice?\\

\begin{enumerate}
  \item \textbf{Software Specification Design}\\
    Revise existing papers on software specification design, for example material on MIS specifications, and work done by David Parnas
    Reflect on previous Mcmaster Software Engineering course on software design (i.e. 3A04)

  \item \textbf{Unit Test Planning}\\
    Revise testing blogs produced by large tech companies such as Meta, Google and Amazon. These blogs devise the strategies these companies adopt in their creation of effective unit tests.
    Reflect on previous Mcmaster Software Engineering course, 3S03 Software Testing

  \item \textbf{Software Design Principles}\\
    Revise SOLID principles via informational blogs such as DigitalOcean, Medium, and FreecodeCamp
    Implement personal projects that are small but correctly implement each of the design principles.

  \item \textbf{Diffusion Model Concepts/Implementation}\\
    Utilize Introduction to Diffusion Models course on Udemy. This goes over theory and implementation in practice today.
    Read existing papers on experiments with diffusion models by other researchers that have similar applications
\end{enumerate}

Now, each team member selected a particular method to pursue, and stated their justifications below:

\textbf{Jared Paul:} I'm going to revise more formal and professional software specification design practices by looking at existing papers, such as that from David Parnas. This is because I recall being introduced to him in 2nd year, and found the content really straightforward and informative the, so I thought it would be able to get me up to speed quite quickly today.

\textbf{Hamza Issa:} I chose to revise existing engineering blogs from big tech companies like Amazon, as although there are papers and lots of theory out there for devising unit tests. I think that in designing and creating the best unit tests we can look to companies that are highly dependent on them, to find tests that are not just complete but not overly verbose.

\textbf{Ahmad Hamadi:} I plan on revising my SOLID principles using popular blogs like I've seen from FreeCode camp. I think this would be the simplest way to learn all the principles in a manner that is easy to digest and also easy to implement myself for when I want to experiment and practice.

\textbf{Harrison Chiu:} I am going to try and complete the Udemy course on Introduction to Diffusion models. I have some experience looking at research papers for Diffusion models, but they have been really difficult to understand due to the advanced mathematics. I think using a Udemy course would make it simpler to learn and easily transferable to our project

\textbf{Gurnoor Bal:} I am going to investigate existing research papers in the Diffusion Models space that have some similarity to our application. I think this would not only help me learn the required concepts, but also show me how to actually produce research and an application catered for this specific project. If i were to just watch the Udemy course, I'll understand the concepts, but I would still need to review papers to understand how to go about it for this particular project.

\end{document}