\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{makecell}
\usepackage{longtable}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
10 March & 1.0 & Gurnoor and Harrison add section 3\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

Table 1, includes the definitions and descriptions of all relevant symbols,
abbreviations and acronyms used in this VnV Plan document.

\begin{longtable}[c]{|p{0.3\textwidth}|p{0.7\textwidth}|}
  \hline
  \textbf{Symbol, Abbreviation or Acronym} & \textbf{Definiton or Description} \\ \hline
  \textbf{ML} & Machine Learning: A branch of artificial intelligence that involves the use of algorithms to allow computers to learn from and make predictions based on data. This is a core technology used in the project for analyzing chest X-rays. \\ \hline
  \textbf{DL} & Deep Learning: A subset of machine learning involving neural networks with many layers, used to analyze various types of data, including images. \\ \hline
  \textbf{DICOM} & Digital Imaging and Communications in Medicine: A standard for transmitting, storing, and sharing medical imaging information. It is used to manage medical images in the proposed solution. \\ \hline
  \textbf{CNN} & Convolutional Neural Network: A type of deep learning model specifically designed for processing structured grid data like images, used in the project for chest X-ray analysis. \\ \hline
  \textbf{EHR} & Electronic Health Record: A digital version of a patient's paper chart, used for storing patient information and history that may be integrated with the proposed solution. \\ \hline
  \textbf{API} & Application Programming Interface: A set of rules and protocols for building and interacting with software applications, enabling the integration of the proposed solution with other systems. \\ \hline
  \textbf{MC} & Mandated Constraints: Various constraints placed on the project’s proposed solution that must be adhered to throughout the development process. \\ \hline
  \textbf{FR} & Functional Requirement: A requirement that specifies what functionality the project’s proposed solution must provide to meet user needs. \\ \hline
  \textbf{NFR} & Nonfunctional Requirement: A requirement that specifies criteria that can be used to judge the operation of a system, rather than specific behaviors (e.g., performance, usability). \\ \hline
  \textbf{BUC} & Business Use Case: A scenario that describes how the proposed solution can be used within a business context to achieve specific goals. \\ \hline
  \textbf{PUC} & Product Use Case: A scenario that details how an individual user will interact with the proposed solution to achieve specific tasks. \\ \hline
  \textbf{MVP} & Minimum Viable Product: A version of the proposed solution that includes only the essential features required to meet the core needs of the users and stakeholders. \\ \hline
  \textbf{MG} & Module Guide \\ \hline
  \textbf{MIS} & Module Interface Specification \\ \hline
  \textbf{PoC} & Proof of Concept \\ \hline
  \textbf{SRS} & Software Requirements Specification \\ \hline
  \textbf{FRTC} & Functional Requirements Test Case \\ \hline
  \textbf{NFRTC} & Nonfunctional Requirements Test Case \\ \hline
  \textbf{VnV} & Verification and Validation \\ \hline
\end{longtable}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document ...

\section{Functional Requirements Evaluation}

\subsection{User Authentication (FR.8)}
\subsubsection{User Authentication (FRTC11, 12, 15, 16, 17 - Updated)}
\textbf{Initial State:} The system is operational with user accounts created.\\
\textbf{Input:} Valid login credentials for an authorized user.\\
\textbf{Expected Output:} The system authenticates the user and grants access according to the user's role.\\
\textbf{Actual Output:} The system successfully authenticates the user and redirects them to the dashboard, allowing appropriate access based on their role.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.8 (Authentication and Authorization Mechanisms)\\

\subsection{Image Upload (FR.1)}
\subsubsection{Chest X-ray Image Input Acceptance (FRTC1)}
\textbf{Initial State:} The system is in a stable state with all components initialized and ready to receive input.\\
\textbf{Input:} A sample chest X-ray image in a valid format (JPG, PNG, DICOM).\\
\textbf{Expected Output:} The system accepts and reads the chest X-ray image successfully.
No error messages or system anomalies occur.\\
\textbf{Actual Output:} The system successfully accepts, reads, and stores the uploaded image in the backend.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.1 (Image Upload)\\

Invalid Chest X-ray Image Format Rejection (FRTC2)
\textbf{Initial State:} The system is ready to receive an image file.\\
\textbf{Input:} A sample image in an invalid format (e.g., .TXT, .DOCX).\\
\textbf{Expected Output:} The system rejects the invalid image input.\\
An appropriate error message is displayed, informing the user about supported formats.
\textbf{Actual Output:} The system correctly rejects invalid file formats and provides an error message.\\
\textbf{Expected and Actual Output Match:} True\\

\subsection{Image Preprocessing (FR.2 - Updated)}
\subsubsection{Image Preprocessing and Standardization (FRTC3 - Completely Revised)}
\textbf{Initial State:} A chest X-ray image has been uploaded and is ready for preprocessing.\\
\textbf{Input:} A valid chest X-ray image in JPG, PNG, or DICOM format.\\
\textbf{Expected Output:} The system resizes the image to match the CNN model’s required dimensions.\\
Pixel intensity values are normalized to enhance consistency.
The preprocessed image is saved and ready for inference.
\textbf{Actual Output:} The system successfully resizes and normalizes images, preparing them for model analysis.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.2 (Image Preprocessing)\\

\subsubsection{Handling Invalid Image Formats (FRTC4 - Updated)}
\textbf{Initial State:} The system is awaiting an image upload.\\
\textbf{Input:} An invalid image file (e.g., TXT, DOCX, unsupported formats, or corrupted image files).\\
\textbf{Expected Output:}
\begin{itemize}
\item The system detects that the image is in an unsupported format or corrupted.
\item The system displays an error message to the user.
\item The system prevents further processing and does not send the image to the model.
\end{itemize}
\textbf{Actual Output:} The system successfully identifies invalid formats and blocks the upload, displaying an appropriate error message.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.2 (Image Preprocessing)\\

\subsection{CNN Model Accurate Analysis (FR.3 - Updated)}
\subsubsection{Disease Classification with Confidence Scores (FRTC5 - Updated)}
\textbf{Initial State:} The system has preprocessed an uploaded image and is ready for analysis.\\
\textbf{Input:} A preprocessed chest X-ray image with known disease patterns (e.g., Pneumonia, Cardiomegaly, Atelectasis).\\
\textbf{Expected Output:}
\begin{itemize}
\item The CNN model classifies the image and assigns a disease label.
\item The model outputs confidence scores (e.g., Pneumonia: 85\%, No Finding: 10\%).
\item The prediction results are displayed to the user.
\end{itemize}
\textbf{Actual Output:} The CNN model successfully classifies diseases and assigns confidence scores.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.3 (CNN Model Accurate Analysis)\\

\subsubsection{Model Prediction for No Disease Cases (FRTC6 - Updated)}
\textbf{Initial State:} The CNN model is ready for analysis, and a healthy chest X-ray is uploaded.\\
\textbf{Input:} A preprocessed chest X-ray image of a patient with no known disease.\\
\textbf{Expected Output:}
\begin{itemize}
\item The CNN model classifies the image as No Finding or returns very low probabilities for disease labels.
\item The confidence scores reflect minimal probability of disease.
\item The prediction results are displayed to the user.
\end{itemize}
\textbf{Actual Output:} The system correctly identifies images without disease and returns a No Finding classification.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.3 (CNN Model Accurate Analysis)\\

\subsection{Display Results (FR.6 - Updated)}
\subsubsection{Diagnostic Report and Heatmap Access via Web Interface (FRTC10 - Updated)}
\textbf{Input:} The user navigates to the results page after an image has been analyzed.\\
\textbf{Expected Output:} The system displays the predicted disease classification(s). The confidence scores are presented next to each disease label. (Optional) A heatmap overlay is shown, visually indicating affected areas in the X-ray. The interface is formatted clearly for easy interpretation.\\
\textbf{Actual Output:} The system correctly displays predicted disease labels, corresponding confidence scores, and a heatmap visualization (if enabled).\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.6 (Display Results)\\

\subsubsection{Display Prediction Results in Readable Format (FRTC18 - New)}
\textbf{Initial State:} The system has completed model inference, and the user is accessing the results page.\\
\textbf{Input:} The user opens the result page after an image has been analyzed.\\
\textbf{Expected Output:} The predicted disease label(s) are prominently displayed. The corresponding confidence scores are formatted for clarity. No extraneous or misleading information is presented. Results are accessible within a reasonable timeframe (<5 seconds).\\
\textbf{Actual Output:} The system successfully displays predictions in a structured and readable format.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.6 (Display Results)\\

\subsection{Heatmap Report (FR.7)}
\subsubsection{Heatmap Display on Chest X-ray Images (FRTC9 - Updated)}
\textbf{Initial State:} The CNN model has completed image analysis, and the system has generated a heatmap.\\
\textbf{Input:} The user navigates to the results page and requests a heatmap visualization.\\
\textbf{Expected Output:} The system overlays a heatmap on the chest X-ray image. The heatmap highlights areas where the model detected abnormalities. The overlay does not obscure critical image details.\\
\textbf{Actual Output:} The system successfully generates and overlays the heatmap on the X-ray image, maintaining image clarity.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.7 (Heatmap Report)\\

\subsubsection{Heatmap Generation Error Handling (FRTC19 - New)}
\textbf{Initial State:} The CNN model has completed analysis, but an error occurs in heatmap generation.\\
\textbf{Input:} The system attempts to generate a heatmap but encounters an issue (e.g., missing heatmap data, computation error).\\
\textbf{Expected Output:} The system displays a message indicating that the heatmap could not be generated. The rest of the diagnostic results (predictions and confidence scores) remain accessible.\\
\textbf{Actual Output:} The system successfully identifies heatmap generation errors and informs the user without affecting other results.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.7 (Heatmap Report)\\

\subsection{User Dashboard (FR.9)}
\subsubsection{Display User Dashboard with Past Uploads (FRTC20 - New)}
\textbf{Initial State:} The user is logged in and has previously uploaded X-ray images.\\
\textbf{Input:} The user navigates to the dashboard page.\\
\textbf{Expected Output:} The system retrieves and displays a list of past uploaded X-ray images. Each entry includes the upload timestamp and corresponding diagnosis results.\\
\textbf{Actual Output:} The system successfully loads and displays past uploads with timestamps and results.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.9 (User Dashboard)\\

\subsection{Secure API for Model Inference (FR.9)}
\subsubsection{API Endpoint for Image Inference (FRTC21 - New)}
\textbf{Initial State:} No request is made, and the API is available.\\
\textbf{Input:} A POST request containing a valid chest X-ray image.\\
\textbf{Expected Output:}
\begin{itemize}
\item The API processes the image and forwards it to the CNN model.
\item The API returns a JSON response with the predicted disease label and confidence scores.
\item The response follows the correct structure (e.g., { "disease": "Pneumonia", "confidence": 85\% }).
\end{itemize}
\textbf{Actual Output:} The API successfully processes the request and returns the correct JSON response format.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.9 (Secure API for Model Inference)\\

\subsubsection{API Handling of Invalid Image Format (FRTC22 - New)}
\textbf{Initial State:} No request is made, and the API is available.\\
\textbf{Input:} A POST request containing an invalid file format (e.g., .TXT, .DOCX).\\
\textbf{Expected Output:}
\begin{itemize}
\item The API rejects the request.
\item The API returns a structured error message (e.g., { "error": "Invalid file format. Please upload \item JPG, PNG, or DICOM." }).
\end{itemize}
\textbf{Actual Output:} The API correctly rejects invalid file formats and returns an appropriate error message.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.9 (Secure API for Model Inference)\\

\subsection{Data Storage \& Management (FR.7)}
\subsubsection{Secure Image and Result Storage (FRTC13 - Updated)}
\textbf{Initial State:} A user has uploaded an X-ray image, and the system is ready to store the data.\\
\textbf{Input:} A valid chest X-ray image is uploaded, and the CNN model returns predictions.\\
\textbf{Expected Output:}
\begin{itemize}
\item The system stores the uploaded image in a secure storage location.
\item The system stores prediction results in the database, linking them to the respective image.
\item No unauthorized access to stored data occurs.
\end{itemize}
\textbf{Actual Output:} The system successfully stores images and results securely, ensuring accessibility for future retrieval.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.7 (Data Storage \& Management)

\subsubsection{Retrieval of Stored Results (FRTC14 - Updated)}
\textbf{Initial State:} The database contains stored images and results for a logged-in user.\\
\textbf{Input:} A user navigates to the dashboard and requests to view past results.\\
\textbf{Expected Output:}
\begin{itemize}
\item The system retrieves and displays stored images with associated predictions.
\item Each entry includes timestamps and confidence scores for past analyses.
\item The data retrieval process is efficient, ensuring a smooth user experience.
\end{itemize}
\textbf{Actual Output:} The system successfully retrieves and displays stored images and results on the dashboard.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.7 (Data Storage \& Management)

\subsection{Error Handling \& Notifications (FR.9)}
\subsubsection{Invalid Image Format Handling (FRTC2 - Updated)}
\textbf{Initial State:} The system is awaiting an image upload.\\
\textbf{Input:} A user attempts to upload an unsupported file format (e.g., TXT, DOCX, MP4).\\
\textbf{Expected Output:}
\begin{itemize}
\item The system rejects the upload.
\item An error message appears stating, “Invalid file format. Please upload a JPG, PNG, or DICOM file.”
\item The user is prompted to try again with a valid file format.
\end{itemize}
\textbf{Actual Output:} The system correctly rejects invalid file formats and displays an appropriate error message.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.9 (Error Handling \& Notifications)

\subsubsection{Corrupted Image Upload Handling (FRTC4 - Updated)}
\textbf{Initial State:} The system is awaiting an image upload.\\
\textbf{Input:} A user attempts to upload a corrupted chest X-ray image.\\
\textbf{Expected Output:}
\begin{itemize}
\item The system detects that the file is corrupted and cannot be processed.
\item An error message appears stating, “File appears to be corrupted. Please upload a valid image.”
\item The system does not store or process the corrupted file.
\end{itemize}
\textbf{Actual Output:} The system successfully detects and rejects corrupted image files, displaying an appropriate error message.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.9 (Error Handling \& Notifications)

\subsection{Multi-Disease Classification (FR.11)}
\subsubsection{Multi-Disease Detection with Confidence Scores (FRTC5 and 6 - Updated)}
\textbf{Initial State:} The system has preprocessed an uploaded image and is ready for analysis.\\
\textbf{Input:} A preprocessed chest X-ray image containing multiple known diseases (e.g., Pneumonia and Tuberculosis).\\
\textbf{Expected Output:}
\begin{itemize}
\item The CNN model classifies the image and returns a list of detected diseases.
\item The confidence scores for each predicted disease are displayed.
\\textbf{item Example output:} Pneumonia: 85\%, Tuberculosis: 70\%.
\end{itemize}
\textbf{Actual Output:} The CNN model successfully detects multiple diseases in a single image and provides confidence scores.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.11 (Multi-Disease Classification)\\

\subsubsection{No Multi-Disease Detection for Single Condition (FRTC5 - Updated)}
\textbf{Initial State:} The system has preprocessed an uploaded image and is ready for analysis.\\
\textbf{Input:} A preprocessed chest X-ray image with only one known disease (e.g., Cardiomegaly).\\
\textbf{Expected Output:}
\begin{itemize}
\item The CNN model classifies the image and returns a single disease label.
\item Confidence scores for unrelated diseases remain low.
\textbf{Example output:} Cardiomegaly: 92\%, No Finding: 8\%.
\end{itemize}
\textbf{Actual Output:} The system correctly identifies only one disease when multiple diseases are not present.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.11 (Multi-Disease Classification)\\

\subsection{Model Confidence (FR.12)}
\subsubsection{Low Confidence Warning for Uncertain Predictions (FRTC8 - Updated)}
\textbf{Initial State:} The system has preprocessed an uploaded image and is ready for analysis.\\
\textbf{Input:} A preprocessed chest X-ray image that leads to low confidence predictions (e.g., all detected diseases have confidence scores below 50\%).
\textbf{Expected Output:}
\begin{itemize}
\item The system returns disease classifications with confidence scores.
\item If the highest confidence score is below 50\%, the system displays a warning: “Low confidence in prediction – consider consulting a radiologist.”
\end{itemize}
\textbf{Actual Output:} The system correctly identifies low-confidence cases and displays the warning message.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.12 (Model Confidence)\\

\subsubsection{High Confidence Predictions Displayed Normally (FRTC7 - Updated)}
\textbf{Initial State:} The system has preprocessed an uploaded image and is ready for analysis.\\
\textbf{Input:} A preprocessed chest X-ray image that leads to high-confidence predictions (e.g., Pneumonia: 85\%).
\textbf{Expected Output:}
\begin{itemize}
\item The system returns disease classifications with confidence scores.
\item If confidence scores are above 50\%, no warning is displayed.
\end{itemize}
\textbf{Actual Output:} The system correctly displays high-confidence predictions without unnecessary warnings.\\
\textbf{Expected and Actual Output Match:} True\\
\textbf{Relevant Functional Requirement(s):} FR.12 (Model Confidence)\\


\section{Nonfunctional Requirements Evaluation}

\subsection{Usability}
		
\subsection{Performance}

\subsection{etc.}
	
\section{Comparison to Existing Implementation}	

\begin{table}[]
  \centering
  \begin{tabular}{|l|l|}
  \hline
  \textbf{Projects} & \textbf{Links} \\ \hline
  This project & \url{https://github.com/harrisonchiu/xray} \\ \hline
  CXR-Capstone & \url{https://github.com/RezaJodeiri/CXR-Capstone} \\ \hline
  CXR-LLaVA & 
  \makecell{\url{https://github.com/ECOFRI/CXR\_LLaVA} \\ \url{https://arxiv.org/abs/2310.18341}} \\ \hline
  \end{tabular}
\end{table}

\subsection{CXR-LLaVA Comparison}
CXR-LLaVA is an open-source multimodal large language model designed to interpret chest X-ray images and generate radiologic reports for those images. By integrating advanced vision transformers with large language models, it aims to emulate the diagnostic capabilities of human radiologists.

Their model was trained on many open source CXR datasets. These include:
\begin{itemize}
\item BrixIA: A dataset focusing on COVID-19 pneumonia severity scoring.​
\item CheXpert: A large dataset with labeled CXR images covering multiple pathologies.
\item MIMIC-CXR: Comprises de-identified CXR images and corresponding radiology reports.​
\item NIH Chest X-ray Dataset: Contains over 100,000 CXR images with annotations for 14 disease labels.
\item PadChest: Offers a wide variety of thoracic disease labels and radiographic findings.​
\item RSNA COVID-19 AI Detection Challenge: Provides CXR images related to COVID-19 cases.​
\item VinDR-CXR: A dataset with annotations for various lung diseases.
\end{itemize}

Collectively, there are more than 500 000 different CXR images which is a lot of training data.

CXR-LLaVA's architecture is made of:​
\begin{itemize}
\item Image Encoder: A Vision Transformer (ViT-L/16) processes CXR images at a resolution of 512x512 pixels in grayscale.​
\item Language Model: LLAMA2-7B-CHAT serves as the foundational language model, facilitating the generation of coherent and contextually relevant radiologic reports.
\end{itemize}

Generally, CXR-LLaVA and our project share the common goal of analysing chest x-ray images to identify diseases. However, they still differ in several aspects.

Our project focuses on detecting diseases in CXR images with a CNN in the backend. CXR-LLaVA on top of that, generates a comprehensive radiologic report from the CXR image, summarizing its findings in order to emulate a proper radiologist diagnosis. As a result, their model architecture is more complex, combining a Vision Transformer for the image encoder and a Llama language model to output a textual report based on the visual input from the vision transformer. Vision transformers and language models are very data hungry. So, they also use a lot more datasets, composed of over 500 000 different images.

However, there is no front end. In order to use it, you must follow their instructions to load their trained model and then run it in the terminal.

It is an advanced research project with its own research paper referencing it.

\subsection{CXR-Capstone}
The CXR-Capstone project is an open-source project that analyzes chest X-ray (CXR) images to predict potential diseases using a CNN model with multi-label prediction. It also uses demographic information such as race, age, and gender. In this way, it classifies images based on demographics as well for the intent of mitigating biases.

The project is very similar to ours as it has a frontend running a backend model that takes in the user’s uploaded image and analyses if there are any diseases. On top of that, it also tracks the progression of conditions over time and generates a simple diagnostic report based on the image.

Their project uses a newer model architecture of DETR (detection transformer). It is said to have higher accuracy and run time performance than traditional CNNs. Its architecture is based on a transformer which is typically more data hungry, so it also uses a much larger dataset, MIMIC-CXR, which has over 300 000 images.

The end product is still similar to our project but has slightly more features of tracking progression (assuming the users continuously upload new images of themselves) and report generation.


\section{Unit Testing}

\section{Changes Due to Testing}

\wss{This section should highlight how feedback from the users and from 
the supervisor (when one exists) shaped the final product.  In particular 
the feedback from the Rev 0 demo to the supervisor (or to potential users) 
should be highlighted.}

\section{Automated Testing}
Automated testing for our project currently emphasizes maintaining high code quality through consistent linting and formatting checks. The existing automated tests ensure code quality, readability, and consistency across the entire codebase. Our project does not require extensive CI/CD automated testing because the team is small and focused. The codebase is also manageable and easily divided up in parts where each member could work on it separately.

\subsection{Linting and Formatting}

We utilize GitHub Actions workflows to automate linting and formatting checks for both frontend and backend components. These automated checks help maintain code consistency, improve readability, and minimize potential bugs.

\subsection{Frontend (React.js / JavaScript)}

We use two primary tools for frontend code quality:
\begin{itemize}
\item \textbf{ESLint:} ESLint analyzes JavaScript and React code, enforcing best practices and identifying errors or potential issues such as unused variables, improper imports, syntax errors, and inconsistent code patterns. Developers run ESLint locally before pushing code to identify and fix issues proactively. The ESLint configuration adheres to industry-standard rules optimized for React development, providing detailed error and warning messages to facilitate rapid fixes.
\item \textbf{Prettier:} Prettier automatically formats JavaScript and React code, enforcing consistent indentation, spacing, bracket placement, and line lengths. This ensures that the frontend codebase remains uniform regardless of individual developer preferences. Automated checks verify adherence to the Prettier rules, failing commits or pull requests that do not comply.
\end{itemize}

\subsection{Backend (Python)}

The backend uses robust automated checks using the following tools:
\begin{itemize}
\item \textbf{Flake8:} Flake8 enforces adherence to Python’s PEP 8 standards. It identifies potential syntax errors, unused imports or variables, and common coding pitfalls. By automating Flake8 checks, we ensure backend code quality remains consistently high, reducing manual code reviews and catching errors early in the development process.
\item \textbf{Black:} Black automatically formats Python code to a unified and clear style, eliminating debates about formatting preferences. It handles line length, spacing, and structural formatting. Automated Black checks verify code conformity and clearly indicate any violations, ensuring readability and maintainability.
\end{itemize}

\subsection{Running Automated Tests Locally}
Developers can perform these tests locally before committing changes, ensuring issues are addressed early:

\subsubsection{JavaScript Checks:}
\verb|npm run lint| and \verb|npm run format:check|
\begin{itemize}
\item This command runs ESLint and Prettier, respectively, highlighting or automatically fixing formatting and linting issues.
\end{itemize}

\verb|flake8 .| and \verb|black --check .|
\begin{itemize}
\item Running these commands validates backend Python code adherence to style and linting standards.
\end{itemize}

\subsection{GitHub Actions Integration}
GitHub Actions workflows trigger these automated checks each time a developer makes a pull request or pushes code to the main branch. These workflows include:
\begin{itemize}
\item Linting Workflow: Runs ESLint (frontend) and Flake8 (backend). If linting errors are found, the workflow fails, clearly identifying issues that must be resolved before merging.
\item Formatting Workflow: Checks formatting with Prettier (frontend) and Black (backend). Any deviations from the established formatting rules trigger a failed workflow, requiring immediate correction by developers.
\end{itemize}

These checks ensure only clean and consistently formatted code enters the main codebase.

The GitHub Actions workflows are configured to trigger automatically whenever:
\begin{itemize}
\item A pull request is opened or updated.
\item Code is pushed to critical branches (e.g., main or development).
\end{itemize}

If a workflow fails, GitHub clearly indicates the exact nature and location of errors, allowing developers to quickly identify and fix issues. This rapid feedback cycle helps team productivity and helps maintain a robust and error-free codebase



		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.

\input{../Reflection.tex}

\begin{enumerate}
  \item What went well while writing this deliverable? 
  \item What pain points did you experience during this deliverable, and how
    did you resolve them?
  \item Which parts of this document stemmed from speaking to your client(s) or
  a proxy (e.g. your peers)? Which ones were not, and why?
  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)
\end{enumerate}

\end{document}